{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2402b02e-f992-4297-85c6-9e5f46a553d9",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3577780a-888e-4e2a-a8ed-8aee9d9ed4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning Management\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Path management\n",
    "import pathlib\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Regular Expressions\n",
    "import re\n",
    "\n",
    "# Logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58f8b5-c816-41fd-bbb1-c99da4d4fc43",
   "metadata": {},
   "source": [
    "## Paths and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7e438f-f3bc-4879-ac7b-29b730b9270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e896fa80-5352-40f6-8b09-6ed53ed4392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_matching_list = pd.read_csv(path.parent / \"all_vins_nhtsa.csv\").drop_duplicates(\"vin_corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8e22d7-f21f-466a-b134-b6df033ee835",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(path.parent / \"2019-21_data_compiled_RN_100323.csv\", header=0, chunksize = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427d79b-02df-45b8-9a30-375c1e75f8fb",
   "metadata": {},
   "source": [
    "## Functions used to process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88915d44-e6fa-4fd5-bddb-bbe548425691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_1970357/729533879.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  matched = re.match(\"^\\s*[0-9]*[0-9]{4}\\.?0?\\s*$\", split_zip[0])\n"
     ]
    }
   ],
   "source": [
    "def check_valid_zip(zip_code):\n",
    "    zip_str = str(zip_code)\n",
    "    split_zip = re.split(\"-\", zip_str)\n",
    "    if len(split_zip) == 2:\n",
    "        if check_valid_zip(split_zip[0]) & check_valid_zip(split_zip[1]):\n",
    "            return 2\n",
    "        elif check_valid_zip(split_zip[0]):\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "    elif len(split_zip) == 1:\n",
    "        # MUST ADDRESS STARTING \"Os\"\n",
    "        # starting_o = re.match('^O', split_zip[0])\n",
    "        matched = re.match(\"^\\s*[0-9]*[0-9]{4}\\.?0?\\s*$\", split_zip[0])\n",
    "        if matched:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def get_valid_zips(zip_df, zip_column_name):\n",
    "    \"\"\"\n",
    "    Inputs: A DataFrame with a column called \"zip\"\n",
    "    Returns: A DataFrame of the same length as the input, with three columns: zip, zip_valid_code, and zip_corrected\n",
    "    \"\"\"\n",
    "    # Prepare the list to be used\n",
    "    zip_list = zip_df[[zip_column_name]].rename(columns = {zip_column_name : \"zip\"}).reset_index(drop = True)\n",
    "\n",
    "    # Get validity code\n",
    "    zip_list.loc[:, \"zip_valid_code\"] = zip_list.loc[:, \"zip\"].apply(lambda x: check_valid_zip(x))\n",
    "\n",
    "    # Get indices\n",
    "    correct_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==1].index\n",
    "    invalid_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==0].index\n",
    "    two_part_zips_indices = zip_list[zip_list[\"zip_valid_code\"]>1].index\n",
    "\n",
    "    zip_list.loc[correct_zips_indices, \"zip_corrected\"] = zip_list.loc[correct_zips_indices, \"zip\"]\n",
    "    zip_list.loc[invalid_zips_indices, \"zip_corrected\"] = np.NaN\n",
    "    zip_list.loc[two_part_zips_indices, \"zip_corrected\"] = zip_list.loc[two_part_zips_indices, \"zip\"].astype(str).str[0:5]\n",
    "\n",
    "    return zip_list[[\"zip_corrected\"]]\n",
    "\n",
    "def convert_vin_valid(vin):\n",
    "    try:\n",
    "        vin_str = str(vin)\n",
    "        if len(vin_str) < 11:\n",
    "            return \"NA\"\n",
    "        if \" \" in vin_str[0:11]:\n",
    "            return \"NA\"\n",
    "        else:\n",
    "            return vin_str[0:8]+\"*\"+vin_str[9:11]\n",
    "    except:\n",
    "        return \"NA\"\n",
    "\n",
    "def return_matched_vins(chunk_number, df, vin_column, matching_list):\n",
    "    match = df.merge(matching_list,\n",
    "                    left_on = vin_column,\n",
    "                    right_on = vin_column,\n",
    "                    how = 'left')\n",
    "    \n",
    "    # Get rows of DF where VINS matched\n",
    "    df_vins_matched = match.loc[match[\"Manufacturer Name\"].notna(), :]\n",
    "    df_vins_unmatched = match.loc[match[\"Manufacturer Name\"].isna(), :]\n",
    "    \n",
    "    # Get length\n",
    "    len_matched = len(df_vins_matched)\n",
    "    len_unmatched = len(df_vins_unmatched)\n",
    "    len_all = len(match)\n",
    "    \n",
    "    # Create df\n",
    "    tally_dict = {\"Chunk Number\": [chunk_number],\n",
    "                  \"Matched\" : [len_matched],\n",
    "                  \"Unmatched\" : [len_unmatched],\n",
    "                  \"All\" : [len_all]}\n",
    "    \n",
    "    match_unmatched_tally = pd.DataFrame(tally_dict)\n",
    "\n",
    "    return [match, match_unmatched_tally]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84944585-867b-4e79-8fc5-8805ef6d3602",
   "metadata": {},
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7505e46b-0914-4e76-82cf-5dd727acaf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 0\n",
      "Currently processing chunk number 10\n",
      "Currently processing chunk number 20\n",
      "Currently processing chunk number 30\n",
      "Currently processing chunk number 40\n",
      "Currently processing chunk number 50\n",
      "Currently processing chunk number 60\n",
      "Currently processing chunk number 70\n",
      "Currently processing chunk number 80\n",
      "Currently processing chunk number 90\n",
      "Currently processing chunk number 100\n",
      "Currently processing chunk number 110\n",
      "Currently processing chunk number 120\n",
      "Currently processing chunk number 130\n",
      "Currently processing chunk number 140\n",
      "Currently processing chunk number 150\n",
      "Currently processing chunk number 160\n",
      "Currently processing chunk number 170\n",
      "Currently processing chunk number 180\n",
      "Currently processing chunk number 190\n",
      "Currently processing chunk number 200\n",
      "Currently processing chunk number 210\n",
      "Currently processing chunk number 220\n",
      "Currently processing chunk number 230\n",
      "Currently processing chunk number 240\n",
      "Currently processing chunk number 250\n",
      "Currently processing chunk number 260\n",
      "Currently processing chunk number 270\n",
      "Currently processing chunk number 280\n",
      "Currently processing chunk number 290\n",
      "Currently processing chunk number 300\n",
      "Currently processing chunk number 310\n",
      "Currently processing chunk number 320\n",
      "Currently processing chunk number 330\n",
      "Currently processing chunk number 340\n",
      "Currently processing chunk number 350\n",
      "Currently processing chunk number 360\n",
      "Currently processing chunk number 370\n",
      "Currently processing chunk number 380\n",
      "Currently processing chunk number 390\n",
      "Currently processing chunk number 400\n",
      "Currently processing chunk number 410\n",
      "Currently processing chunk number 420\n",
      "Currently processing chunk number 430\n",
      "Currently processing chunk number 440\n",
      "Currently processing chunk number 450\n",
      "Currently processing chunk number 460\n",
      "Currently processing chunk number 470\n",
      "Currently processing chunk number 480\n",
      "Currently processing chunk number 490\n",
      "Currently processing chunk number 500\n",
      "Currently processing chunk number 510\n",
      "Currently processing chunk number 520\n",
      "Currently processing chunk number 530\n",
      "Currently processing chunk number 540\n",
      "Currently processing chunk number 550\n",
      "Currently processing chunk number 560\n",
      "Currently processing chunk number 570\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame([])\n",
    "output_tally_df = pd.DataFrame([])\n",
    "i = 0\n",
    "num_to_run = 590\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Check that we haven't gone too far\n",
    "    if i >= num_to_run:\n",
    "        break\n",
    "    \n",
    "    # Display Progress\n",
    "    if (i % 10 == 0):\n",
    "        print(f\"Currently processing chunk number {i}\")\n",
    "    \n",
    "    # Log progress\n",
    "    # logging.info(f\"Chunk Number {self.chunk_number}: commencing processing\")\n",
    "    # logging.info(f\"Chunk Number {self.chunk_number}: chunk length is {len(chunk)}\")\n",
    "\n",
    "    # Reduce reduce the number of columns\n",
    "    chunk_simplified = chunk[['record_from', 'name', 'street', 'city', \n",
    "                          'state', 'zip', 'vehicle_year', 'vehicle_make', 'vehicle_model',\n",
    "                          'vehicle_class', 'vehicle_id']].reset_index(drop = True)\n",
    "\n",
    "    # Get corrected zips\n",
    "    corrected_zip_codes = get_valid_zips(chunk_simplified, \"zip\").reset_index(drop = True)\n",
    "    chunk_simplified = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\n",
    "\n",
    "    # Get the vin codes, and add them as a column\n",
    "    chunk_simplified[\"vin_corrected\"] = chunk_simplified[\"vehicle_id\"].apply(lambda x: convert_vin_valid(x))\n",
    "    chunk_simplified = chunk_simplified.reset_index(drop=True)\n",
    "\n",
    "    # Get the match and tally\n",
    "    matches, tally = return_matched_vins(i, chunk_simplified, \"vin_corrected\", vin_matching_list)\n",
    "\n",
    "    # Concatenate\n",
    "    output_df = pd.concat([output_df, matches])\n",
    "    output_tally_df = pd.concat([output_tally_df, tally])\n",
    "    \n",
    "    # Save every 100\n",
    "    if ((i % 100)+1 ==100) & (i>0):\n",
    "        output_df.to_csv(path.parent / \"processed_chunks_new_new\" / f\"matching_output{i}.csv\")\n",
    "        output_tally_df.to_csv(path.parent / \"processed_chunks_new_new\" / f\"tally_output{i}.csv\")\n",
    "        output_df = pd.DataFrame([])\n",
    "        output_tally_df = pd.DataFrame([])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Final save\n",
    "output_df.to_csv(path.parent / \"processed_chunks_new_new\" / f\"matching_output{i}.csv\")\n",
    "output_tally_df.to_csv(path.parent / \"processed_chunks_new_new\" / f\"tally_output{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd4588-3b1c-4893-bda2-68abb697d160",
   "metadata": {},
   "source": [
    "# Concatenate all the joined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa36de8e-47e2-4c10-a096-10eb88b6516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5787151\n"
     ]
    }
   ],
   "source": [
    "# Check length - this is the same as the legnth of\n",
    "length = 0\n",
    "for file in processed_chunks_path.iterdir():\n",
    "    if file.name[0:8] == \"matching\":\n",
    "        df = pd.read_csv(file, usecols = [\"record_from\"])\n",
    "        length += len(df)\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc77ca55-437f-4a8a-983c-c8567495d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching_output299.csv\n",
      "matching_output199.csv\n",
      "matching_output499.csv\n",
      "matching_output579.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1970357/4224059681.py:7: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, usecols = [\"record_from\", \"zip_corrected\", \"Fuel Type - Primary\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching_output399.csv\n",
      "matching_output99.csv\n"
     ]
    }
   ],
   "source": [
    "processed_chunks_path = path.parent / \"processed_chunks_new_new\" \n",
    "concat_grouped = pd.DataFrame([])\n",
    "cols = 0\n",
    "\n",
    "for file in processed_chunks_path.iterdir():\n",
    "    if file.name[0:8] == \"matching\":\n",
    "        # Track progress\n",
    "        print(file.name)\n",
    "\n",
    "        # Extract the file and create a counter\n",
    "        df = pd.read_csv(file, usecols = [\"record_from\", \"zip_corrected\", \"Fuel Type - Primary\"])\n",
    "        df[\"count\"] = 1\n",
    "        grouped = df.groupby(by=[\"record_from\", \"zip_corrected\", \"Fuel Type - Primary\"]).sum().reset_index()[[\"record_from\", \"zip_corrected\", \"Fuel Type - Primary\", \"count\"]]\n",
    "        grouped = grouped.astype({'zip_corrected': 'str',\n",
    "                                  'Fuel Type - Primary' : 'str',\n",
    "                                  'count': 'int'})\n",
    "        concat_grouped = pd.concat([concat_grouped, grouped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d4c37d-990d-45dc-b0ab-8c61bee4ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_grouped.to_csv(path.parent / \"mun_nv_reg_zip_type.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd466112-4049-4318-b97f-95cfd237b504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gasoline', 'Diesel', 'Electric', 'Flexible Fuel Vehicle (FFV)',\n",
       "       'Not Applicable', 'Ethanol (E85)', 'Compressed Natural Gas (CNG)',\n",
       "       'Liquefied Petroleum Gas (propane or LPG)', 'Natural Gas',\n",
       "       'Fuel Cell'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_grouped[\"Fuel Type - Primary\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdfdb4-905b-47a2-aaaf-c2176495bc6b",
   "metadata": {},
   "source": [
    "# Observe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2326bbed-b856-47b7-b783-2d64d73b38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_zip_mask = (concat_grouped[\"zip_corrected\"].str[:-2].str.zfill(5).str[0:2])==\"06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8e75a4d-6838-498e-8dd9-7dea0d0ed8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_entries = concat_grouped[ct_zip_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96aada04-0069-4c3d-8920-b53064799486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4571268"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_entries[\"count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26cd0fdf-0ed2-4997-9f40-283485d49259",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_2021_mask = ct_entries[\"record_from\"].str.contains(\"_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5a46aed-6173-4a91-9768-15a58cf55284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13674"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_by_type_zip_21 = ct_entries[year_2021_mask].groupby([\"zip_corrected\", \"Fuel Type - Primary\"]).sum().reset_index()\n",
    "evs_21 = ct_by_type_zip_21[ct_by_type_zip_21[\"Fuel Type - Primary\"] == \"Electric\"][\"count\"].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
