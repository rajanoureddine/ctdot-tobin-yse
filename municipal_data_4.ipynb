{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "76e565d8-6f28-4429-ae1a-70c5a00d8e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "# import usaddress\n",
    "\n",
    "from difflib import get_close_matches as clmatch\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely import Point\n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "\n",
    "path = pathlib.Path().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9e606a5c-670c-41a1-8f12-c2c8d00e3739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = path.parent \n",
    "\n",
    "processed_chunks_path = path.parent / \"chunks_new\" \n",
    "\n",
    "processed_chunks_save_path = path.parent / \"processed_chunks_new_save\"\n",
    "\n",
    "matching_list_path = path.parent \n",
    "\n",
    "logging_path = path.parent\n",
    "\n",
    "raw_data = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", chunksize = 1000)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "11f6ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = logging_path / \"new_chunk_match.log\", level = logging.INFO,  format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb256647",
   "metadata": {},
   "source": [
    "# Full processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0381bf6c",
   "metadata": {},
   "source": [
    "## Prepare data and import matching sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c37a08",
   "metadata": {},
   "source": [
    "## Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ad7cc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:135: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:135: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_1740431/4223072178.py:135: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  matched = re.match(\"^\\s*[0-9]*[0-9]{4}\\.?0?\\s*$\", split_zip[0])\n"
     ]
    }
   ],
   "source": [
    "class ChunkProcessor():\n",
    "    def __init__(self, chunks, \n",
    "                 processed_chunk_path, \n",
    "                 number_to_run, \n",
    "                 vin_matcher, zip_column_name, vin_column_name):\n",
    "        self.chunks = chunks\n",
    "        self.processed_chunks = pd.DataFrame([])\n",
    "        self.processed_chunks_path = processed_chunk_path\n",
    "        \n",
    "        self.matched_chunks = pd.DataFrame([])\n",
    "        self.unmatched_chunks = pd.DataFrame([])\n",
    "        self.tally = pd.DataFrame([])\n",
    "        \n",
    "        self.number_to_run = number_to_run\n",
    "        self.chunk_number = 0\n",
    "        self.vin_matcher = vin_matcher\n",
    "        self.zip_column_name = zip_column_name\n",
    "        self.vin_column_name = vin_column_name\n",
    "        \n",
    "    def run(self):\n",
    "        for chunk in self.chunks:\n",
    "            if self.chunk_number < self.number_to_run:\n",
    "                self.process_chunk(chunk)\n",
    "            else:\n",
    "                logging.info(f\"Concluding on chunk number {self.chunk_number}\")\n",
    "                break\n",
    "    \n",
    "    def process_chunk(self, chunk):\n",
    "        try:\n",
    "            # Display Progress\n",
    "            if self.chunk_number % 10 == 0:\n",
    "                print(f\"Currently processing chunk number {self.chunk_number}\")\n",
    "            \n",
    "            # Log progress\n",
    "            logging.info(f\"Chunk Number {self.chunk_number}: commencing processing\")\n",
    "            logging.info(f\"Chunk Number {self.chunk_number}: chunk length is {len(chunk)}\")\n",
    "        \n",
    "            # Reduce reduce the number of columns\n",
    "            chunk_simplified = chunk[['record_from', 'name', 'street', 'city', \n",
    "                                  'state', 'zip', 'vehicle_year', 'vehicle_make', 'vehicle_model',\n",
    "                                  'vehicle_class', 'vehicle_id']].reset_index(drop = True)\n",
    "        \n",
    "            # Get the corrected zip codes, and add them as a column\n",
    "            corrected_zip_codes = self.get_valid_zips(chunk_simplified, self.zip_column_name).reset_index(drop = True)\n",
    "            chunk_simplified = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\n",
    "\n",
    "            # Get two dataframes: matched vins, and unmatched vins, and a tally\n",
    "            # Does NOT change the index \n",
    "            matched_vins, unmatched_vins, tally = self.vin_matcher.match_vins_simple(chunk_simplified,\n",
    "                                                                                    self.vin_column_name,\n",
    "                                                                                   self.chunk_number)\n",
    "            # Join to the original data, keeping original indices\n",
    "            matched_vins = chunk_simplified.join(matched_vins)\n",
    "            unmatched_vins = chunk_simplified.join(unmatched_vins)\n",
    "            \n",
    "            # Confirm tally. This number should be the chunk length\n",
    "            tally[\"confirm_matched\"] = len(matched_vins)\n",
    "            tally[\"confirm_unmatched\"] = len(unmatched_vins)\n",
    "            \n",
    "            # Aggregate\n",
    "            self.aggregate_save_matched_unmatched(matched_vins,\n",
    "                                                 unmatched_vins,\n",
    "                                                 tally)            \n",
    "            \n",
    "            # matched_vins = self.vin_matcher.match_vins(chunk, self.vin_column_name).reset_index(drop=True)\n",
    "\n",
    "            # chunk_processed = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\n",
    "            # chunk_processed = chunk_processed.join(matched_vins)\n",
    "            \n",
    "            # Aggregate and save\n",
    "            # self.aggregate_save_chunk(chunk_processed)\n",
    "\n",
    "            # Update chunk number\n",
    "            self.chunk_number +=1\n",
    "            \n",
    "            # return chunk_processed\n",
    "        except Exception as e:\n",
    "            # THIS IS THE CAUSE - if something falls through, we skip the rest of the chunk. \n",
    "            logging.error(f\"Error encountered on chunk {self.chunk_number}, this means the last chunk to be run was chunk {self.chunk_number-1}\")\n",
    "            logging.error(e)\n",
    "            print(f\"Error encountered on chunk {self.chunk_number}, this means the last chunk to be run was chunk {self.chunk_number-1}\")\n",
    "            print(e)\n",
    "            self.chunk_number +=1\n",
    "        \n",
    "    def aggregate_save_chunk(self, processed_chunk):        \n",
    "        # Add it to the master DF\n",
    "        self.processed_chunks = pd.concat([self.processed_chunks, processed_chunk])\n",
    "        \n",
    "        if ((self.chunk_number % 10)+1 ==10) & (self.chunk_number>0):\n",
    "            logging.info(f\"Saving aggregated chunks numbers {self.chunk_number-10} - {self.chunk_number}\")\n",
    "            dt_string = now.strftime(\"%d%m%y_%H%M\")\n",
    "            self.processed_chunks.to_csv(self.processed_chunks_path / f\"chunk_number_{self.chunk_number}_{dt_string}.csv\")\n",
    "        \n",
    "        # Once saved, reset the DF\n",
    "        self.processed_chunks = pd.DataFrame([])\n",
    "        \n",
    "    def aggregate_save_matched_unmatched(self, matched_chunk, unmatched_chunk, tally): \n",
    "        \"\"\"Input: A DataFrame of data for matched vins, a DF for unmatched VINS, and a tally.\n",
    "        Aggregates this data over 10 chunks, then saves a file\n",
    "        \"\"\"\n",
    "        # Append data to three separate master DFs\n",
    "        self.matched_chunks = pd.concat([self.matched_chunks, matched_chunk])\n",
    "        self.unmatched_chunks = pd.concat([self.unmatched_chunks, unmatched_chunk])\n",
    "        self.tally = pd.concat([self.tally, tally])\n",
    "        \n",
    "        # Every 10th chunk, save\n",
    "        if ((self.chunk_number % 100)+1 ==100) & (self.chunk_number>0):\n",
    "            # Log\n",
    "            logging.info(f\"Saving chunk {self.chunk_number}\")\n",
    "            \n",
    "            # Save the three DataFrames\n",
    "            self.matched_chunks.to_csv(self.processed_chunks_path / f\"matched_chunk_number_{self.chunk_number}.csv\")\n",
    "            self.unmatched_chunks.to_csv(self.processed_chunks_path / f\"unmatched_chunk_number_{self.chunk_number}.csv\")\n",
    "            self.tally.to_csv(self.processed_chunks_path / f\"tally_chunk_number_{self.chunk_number}.csv\")\n",
    "\n",
    "            # Once saved, reset the DF\n",
    "            self.matched_chunks = pd.DataFrame([])\n",
    "            self.unmatched_chunks = pd.DataFrame([])\n",
    "            self.tally = pd.DataFrame([])\n",
    "            \n",
    "            \n",
    "    def check_valid_zip(self, zip_code):\n",
    "        zip_str = str(zip_code)\n",
    "        split_zip = re.split(\"-\", zip_str)\n",
    "        if len(split_zip) == 2:\n",
    "            if self.check_valid_zip(split_zip[0]) & self.check_valid_zip(split_zip[1]):\n",
    "                return 2\n",
    "            elif self.check_valid_zip(split_zip[0]):\n",
    "                return 3\n",
    "            else:\n",
    "                return 0\n",
    "        elif len(split_zip) == 1:\n",
    "            # MUST ADDRESS STARTING \"Os\"\n",
    "            # starting_o = re.match('^O', split_zip[0])\n",
    "            matched = re.match(\"^\\s*[0-9]*[0-9]{4}\\.?0?\\s*$\", split_zip[0])\n",
    "            if matched:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def get_valid_zips(self, zip_df, zip_column_name):\n",
    "        \"\"\"\n",
    "        Inputs: A DataFrame with a column called \"zip\"\n",
    "        Returns: A DataFrame of the same length as the input, with three columns: zip, zip_valid_code, and zip_corrected\n",
    "        \"\"\"\n",
    "        # Prepare the list to be used\n",
    "        zip_list = zip_df[[zip_column_name]].rename(columns = {zip_column_name : \"zip\"}).reset_index(drop = True)\n",
    "\n",
    "        # Get validity code\n",
    "        zip_list.loc[:, \"zip_valid_code\"] = zip_list.loc[:, \"zip\"].apply(lambda x: self.check_valid_zip(x))\n",
    "\n",
    "        # Get indices\n",
    "        correct_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==1].index\n",
    "        invalid_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==0].index\n",
    "        two_part_zips_indices = zip_list[zip_list[\"zip_valid_code\"]>1].index\n",
    "\n",
    "        zip_list.loc[correct_zips_indices, \"zip_corrected\"] = zip_list.loc[correct_zips_indices, \"zip\"]\n",
    "        zip_list.loc[invalid_zips_indices, \"zip_corrected\"] = np.NaN\n",
    "        zip_list.loc[two_part_zips_indices, \"zip_corrected\"] = zip_list.loc[two_part_zips_indices, \"zip\"].astype(str).str[0:5]\n",
    "\n",
    "        return zip_list[[\"zip_corrected\"]]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "05385113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VINMatcher():\n",
    "    def __init__(self, initial_matching_list):\n",
    "        self.matching_list = initial_matching_list.drop_duplicates(\"vin_corrected\")\n",
    "        self.queries = 0\n",
    "        \n",
    "    def valid_vins(self, vins_to_correct_df, vin_column):\n",
    "        \"\"\"\n",
    "        Input: A DataFrame containing a \"vehicle_id\" column of VINs to be corrected\n",
    "        Output: A DataFrame with two columns: \"vehicle_id\" and \"vin_corrected\"\n",
    "        \"\"\"\n",
    "        # Set up\n",
    "        vin_list = vins_to_correct_df        \n",
    "\n",
    "        # Check all alphanumerics\n",
    "        # THIS IS WHERE THE ERROR IS - SOMETIMES HIT A BLANK VIN AND THEREFORE THIS FAILS\n",
    "        # AS IT IS NOT A STRING\n",
    "        # I HAVE APPLIED A STRING OPERATION WITHOUT KNOWING IF STRING FIRST\n",
    "        not_na_mask = vin_list[vin_column].notna()\n",
    "        na_mask = vin_list[vin_column].isna()\n",
    "        \n",
    "        vin_list.loc[not_na_mask, \"vin_alnum_check\"] = vin_list.loc[not_na_mask, vin_column].astype(str).str.strip().str.isalnum()\n",
    "        vin_list.loc[not_na_mask, \"vin_len_check\"] = vin_list.loc[not_na_mask, vin_column].astype(str).str.len() >= 11\n",
    "        \n",
    "        vin_list.loc[na_mask, \"vin_alnum_check\"] = False\n",
    "        vin_list.loc[na_mask, \"vin_len_check\"] = False\n",
    "        \n",
    "        vin_list[\"vin_check\"] = vin_list[\"vin_alnum_check\"] & vin_list[\"vin_len_check\"]\n",
    "\n",
    "        # Create 11-long vins  \n",
    "        vin_list[\"prepared_vins\"] = np.NaN\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"prepared_vins\"] = vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index,vin_column].astype(str).str[0:8]+\"*\"+vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index,vin_column].astype(str).str[9:11]\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==False].index, \"vin_corrected\"] = np.NaN\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"vin_corrected\"] = vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"prepared_vins\"]\n",
    "\n",
    "        # Clean up\n",
    "        vin_list = vin_list.drop(\"prepared_vins\", axis = 1)\n",
    "        vin_list = vin_list[[\"vin_corrected\"]]\n",
    "\n",
    "        # Return\n",
    "        return vin_list.reset_index(drop = True)\n",
    "    \n",
    "    def match_vins_simple(self, df, vin_column, chunk_number):\n",
    "        # Get a list of valid VINs\n",
    "        valid_vin_list = self.valid_vins(df[[vin_column]], vin_column)\n",
    "        \n",
    "        # Attempt a match\n",
    "        match = valid_vin_list.merge(self.matching_list,\n",
    "                                     left_on = \"vin_corrected\",\n",
    "                                     right_on = \"vin_corrected\",\n",
    "                                     how = 'left')\n",
    "        \n",
    "        # Get rows of DF where VINS matched\n",
    "        df_vins_matched = match.loc[match[\"Manufacturer Name\"].notna(), :]\n",
    "        df_vins_unmatched = match.loc[match[\"Manufacturer Name\"].isna(), :]\n",
    "        \n",
    "        # Get length\n",
    "        len_matched = len(df_vins_matched)\n",
    "        len_unmatched = len(df_vins_unmatched)\n",
    "        \n",
    "        # Create df\n",
    "        tally_dict = {\"Chunk Number\": [chunk_number],\n",
    "                      \"Matched\" : [len_matched],\n",
    "                      \"Unmatched\" : [len_unmatched]}\n",
    "        \n",
    "        match_unmatched_tally = pd.DataFrame(tally_dict)\n",
    "        \n",
    "        return [df_vins_matched, df_vins_unmatched, match_unmatched_tally]\n",
    "        \n",
    "    def match_vins(self, df, vin_column):\n",
    "        \"\"\"\n",
    "        Input: A df containing vin columns, that are then corrected, and matched\n",
    "        Returns: matched vins, updated matching list    \n",
    "        \"\"\"\n",
    "        # Get a list of valid VINs\n",
    "        valid_vin_list = self.valid_vins(df[[vin_column]], vin_column)\n",
    "        \n",
    "        # Attempt a match\n",
    "        match = valid_vin_list.merge(self.matching_list,\n",
    "                                     left_on = \"vin_corrected\",\n",
    "                                     right_on = \"vin_corrected\",\n",
    "                                     how = 'left')\n",
    "                                      \n",
    "                                      \n",
    "                                      \n",
    "        # Get unique unmatched vins\n",
    "        unmatched_vins = list(match[match[\"Manufacturer Name\"].isna()][\"vin_corrected\"].unique())\n",
    "\n",
    "        # Print how many\n",
    "        logging.info(f\"VIN matching: a total of {len(unmatched_vins)} VINs were not matched\")\n",
    "\n",
    "        # Variables to download\n",
    "        variables = [\"Manufacturer Name\", \"Model\", \"Model Year\", \"Fuel Type - Primary\", \"Electrification Level\"]\n",
    "        \n",
    "        # Go get them\n",
    "        for vin in tqdm(unmatched_vins):\n",
    "            try:\n",
    "                # Try to fetch the unmatched vin\n",
    "                resp_df = self.fetch_unmatched_vin(vin).reset_index(drop=True)\n",
    "                for variable in variables:\n",
    "                    match.loc[match[match[\"vin_corrected\"]==vin].index, match.columns.isin([variable])] = resp_df[variable][0]\n",
    "                \n",
    "            except BaseException as e:\n",
    "                logging.info(e)\n",
    "                pass\n",
    "\n",
    "        remaining_unmatched = list(match[match[\"Manufacturer Name\"].isna()][\"vin_corrected\"].unique())\n",
    "        \n",
    "        logging.info(f\"VIN Matching: this number of unmatched VINs was reduced by {len(unmatched_vins) - len(remaining_unmatched)}\")\n",
    "        logging.info(f\"VIN Matching: remaining unmatched VINs is {len(remaining_unmatched)}\")\n",
    "\n",
    "        return match\n",
    "    \n",
    "    def fetch_unmatched_vin(self, unmatched_vin):\n",
    "        \"\"\"\n",
    "        Input: An unmatched, but corrected VIN\n",
    "        Output: A matched VIN or NA\n",
    "        \n",
    "        \"\"\"\n",
    "        logging.info(f\"Now matching unmatched vin number {self.queries}: {unmatched_vin}\")\n",
    "        \n",
    "        # Increment the number of times queried\n",
    "        self.queries +=1\n",
    "\n",
    "        \n",
    "\n",
    "        try:\n",
    "            url = (f\"https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVin/{unmatched_vin}?format=csv\")\n",
    "            resp_df = pd.read_csv(url)\n",
    "\n",
    "            # Extract required variables\n",
    "            variables = [\"Manufacturer Name\", \"Model\", \"Model Year\", \"Fuel Type - Primary\", \"Electrification Level\"]\n",
    "            resp_df = resp_df.loc[resp_df[\"variable\"].isin(variables), [\"variable\", \"value\"]].T\n",
    "            resp_df.columns = resp_df.iloc[0]\n",
    "            resp_df = resp_df.drop(\"variable\", axis = 0)\n",
    "            resp_df[\"vin_corrected\"] = unmatched_vin\n",
    "            valid_response = not(resp_df[\"Fuel Type - Primary\"].isna().iloc[0])\n",
    "\n",
    "            # Log whether or not the response is valid\n",
    "            if valid_response:\n",
    "                logging.info(f\"Successfully matched vin number {self.queries}: {unmatched_vin}\")\n",
    "            else:\n",
    "                logging.info(f\"Failed to match vin number {self.queries}: {unmatched_vin}\")\n",
    "            \n",
    "            # Update the matching list\n",
    "            self.matching_list = pd.concat([self.matching_list, resp_df]).reset_index(drop = True)\n",
    "                \n",
    "        except BaseException as e:\n",
    "            msg = f\"Failed to download csv for in number {self.queries}: {unmatched_vin}\"\n",
    "            \n",
    "            logging.info(msg)\n",
    "            logging.info(e)\n",
    "            \n",
    "            print(msg)\n",
    "            print(e)\n",
    "\n",
    "        \n",
    "        # If the number of queries is an increment of 100, save the matching list        \n",
    "        if self.queries % 100 == 0:\n",
    "            logging.info(f\"Saving matching list after {self.queries} queries\")\n",
    "            self.matching_list.to_csv(matching_list_path / f\"matching_list_{self.queries}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284582b",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a8f608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks_path\n",
    "number_to_run = 5900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8ccdbc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajan\\AppData\\Local\\Temp\\ipykernel_14972\\1811061293.py:2: DtypeWarning: Columns (7,9,10,13,20,25,30,34,42,44,56,58,68,78,80,114,115,138,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nhtsa_cleaned = pd.read_csv(path / \"ignored-data\" / \"NHTSA_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load NHTSA data\n",
    "nhtsa_cleaned = pd.read_csv(path / \"ignored-data\" / \"NHTSA_cleaned.csv\")\n",
    "\n",
    "# Simplify the cleaned file\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned[[\"VIN\", \"Manufacturer\", \"Model\", \"ModelYear\", \"FuelTypePrimary\", \"ElectrificationLevel\"]]\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned_simple.rename(columns = {\"VIN\":\"vin_corrected\",\n",
    "                                                              \"Manufacturer\" : \"Manufacturer Name\",\n",
    "                                                              \"ModelYear\" : \"Model Year\",\n",
    "                                                              \"FuelTypePrimary\" : \"Fuel Type - Primary\",\n",
    "                                                              \"ElectrificationLevel\" : \"Electrification Level\"})\n",
    "matching_list = pd.read_csv(matching_list_path / \"matching_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7ffd3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", header=0, chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a08e9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = VINMatcher(nhtsa_cleaned_simple)\n",
    "cp = ChunkProcessor(chunks, processed_chunks_path, number_to_run, vm, \"zip\", \"vehicle_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "462cc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_2 = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", header=0, chunksize = 1000, skiprows=range(1,2930000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "111b1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_2 = VINMatcher(nhtsa_cleaned_simple)\n",
    "cp_2 = ChunkProcessor(chunks_2, processed_chunks_path, number_to_run, vm_2, \"zip\", \"vehicle_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3d4e3665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 0\n",
      "Currently processing chunk number 10\n",
      "Currently processing chunk number 20\n",
      "Currently processing chunk number 30\n",
      "Currently processing chunk number 40\n",
      "Currently processing chunk number 50\n",
      "Currently processing chunk number 60\n",
      "Currently processing chunk number 70\n",
      "Currently processing chunk number 80\n",
      "Currently processing chunk number 90\n",
      "Currently processing chunk number 100\n",
      "Currently processing chunk number 110\n",
      "Currently processing chunk number 120\n",
      "Currently processing chunk number 130\n",
      "Currently processing chunk number 140\n",
      "Currently processing chunk number 150\n",
      "Currently processing chunk number 160\n",
      "Currently processing chunk number 170\n",
      "Currently processing chunk number 180\n",
      "Currently processing chunk number 190\n",
      "Currently processing chunk number 200\n",
      "Currently processing chunk number 210\n",
      "Currently processing chunk number 220\n",
      "Currently processing chunk number 230\n",
      "Currently processing chunk number 240\n",
      "Currently processing chunk number 250\n",
      "Currently processing chunk number 260\n",
      "Currently processing chunk number 270\n",
      "Currently processing chunk number 280\n",
      "Currently processing chunk number 290\n",
      "Currently processing chunk number 300\n",
      "Currently processing chunk number 310\n",
      "Currently processing chunk number 320\n",
      "Currently processing chunk number 330\n",
      "Currently processing chunk number 340\n",
      "Currently processing chunk number 350\n",
      "Currently processing chunk number 360\n",
      "Currently processing chunk number 370\n",
      "Currently processing chunk number 380\n",
      "Currently processing chunk number 390\n",
      "Currently processing chunk number 400\n",
      "Currently processing chunk number 410\n",
      "Currently processing chunk number 420\n",
      "Currently processing chunk number 430\n",
      "Currently processing chunk number 440\n",
      "Currently processing chunk number 450\n",
      "Currently processing chunk number 460\n",
      "Currently processing chunk number 470\n",
      "Currently processing chunk number 480\n",
      "Currently processing chunk number 490\n",
      "Currently processing chunk number 500\n",
      "Currently processing chunk number 510\n",
      "Currently processing chunk number 520\n",
      "Currently processing chunk number 530\n",
      "Currently processing chunk number 540\n",
      "Currently processing chunk number 550\n",
      "Currently processing chunk number 560\n",
      "Currently processing chunk number 570\n",
      "Currently processing chunk number 580\n",
      "Currently processing chunk number 590\n",
      "Currently processing chunk number 600\n",
      "Currently processing chunk number 610\n",
      "Currently processing chunk number 620\n",
      "Currently processing chunk number 630\n",
      "Currently processing chunk number 640\n",
      "Currently processing chunk number 650\n",
      "Currently processing chunk number 660\n",
      "Currently processing chunk number 670\n",
      "Currently processing chunk number 680\n",
      "Currently processing chunk number 690\n",
      "Currently processing chunk number 700\n",
      "Currently processing chunk number 710\n",
      "Currently processing chunk number 720\n",
      "Currently processing chunk number 730\n",
      "Currently processing chunk number 740\n",
      "Currently processing chunk number 750\n",
      "Currently processing chunk number 760\n",
      "Currently processing chunk number 770\n",
      "Currently processing chunk number 780\n",
      "Currently processing chunk number 790\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [249]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcp_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [224]\u001b[0m, in \u001b[0;36mChunkProcessor.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_to_run:\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcluding on chunk number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [224]\u001b[0m, in \u001b[0;36mChunkProcessor.process_chunk\u001b[1;34m(self, chunk)\u001b[0m\n\u001b[0;32m     58\u001b[0m tally[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfirm_unmatched\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unmatched_vins)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Aggregate\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_save_matched_unmatched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatched_vins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43munmatched_vins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtally\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# matched_vins = self.vin_matcher.match_vins(chunk, self.vin_column_name).reset_index(drop=True)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# chunk_processed = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Update chunk number\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [224]\u001b[0m, in \u001b[0;36mChunkProcessor.aggregate_save_matched_unmatched\u001b[1;34m(self, matched_chunk, unmatched_chunk, tally)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Save the three DataFrames\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatched_chunks\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_chunks_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatched_chunk_number_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munmatched_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_chunks_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munmatched_chunk_number_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_number\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtally\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_chunks_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtally_chunk_number_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Once saved, reset the DF\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3554\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3556\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:315\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    312\u001b[0m data \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39miget_values(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mitems))]\n\u001b[0;32m    314\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 315\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\writers.pyx:55\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cp_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f981c8-bae9-4ce0-bad7-72d9b4d65690",
   "metadata": {},
   "source": [
    "# Get VINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "232b6640-fc38-45c9-ae66-d2392895fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_filepaths = [file.name for file in processed_chunks_path.iterdir() if file.name[0:2]==\"un\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6fdac-fc1d-4397-95e1-44e94cff8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_lookup_table_new = pd.DataFrame()\n",
    "all_unmatched_vins = pd.DataFrame()\n",
    "\n",
    "for file_name in tqdm(unmatched_filepaths):\n",
    "    f = pd.read_csv(processed_chunks_path / file_name)\n",
    "    vins = f[\"vin_corrected\"].unique()\n",
    "    df = pd.DataFrame({\"file\": [file_name] * len(vins),\n",
    "                       \"vin_corrected\":vins})\n",
    "    all_unmatched_vins = pd.concat([all_unmatched_vins, df])\n",
    "\n",
    "all_unmatched_vins = all_unmatched_vins.drop_duplicates(subset = \"vin_corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0721f2f6-9d79-45d0-9127-3d3b09cdb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unmatched_vins.to_csv(path.parent / \"all_unmatched_vins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f307dd2-a778-44b5-b7d6-66907546bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unmatched_vins =pd.read_csv(path.parent / \"all_unmatched_vins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9f0903a-713b-43c9-93b8-864fb66623ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unmatched_vins = all_unmatched_vins.drop(\"Unnamed: 0\", axis =1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "640ae128-c97d-4a5f-ae34-c2d27bca0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unmatched_vins_list = all_unmatched_vins[\"vin_corrected\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f29f73-eb94-47ea-8240-b66d7e07df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_unmatched_vins_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a37e6e60-745b-45cc-9278-4a4cf255cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = VINMatcher(pd.DataFrame([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce70f3c-e8b4-4321-88c3-4825afda4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for unmatched_vin in tqdm(all_unmatched_vins_list):\n",
    "    vm.fetch_unmatched_vin(unmatched_vin.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95be1f8-cc40-4411-9a94-fb6760b22711",
   "metadata": {},
   "source": [
    "# Merge with nhtsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fea69619-253f-4399-a84c-bcb00e26403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1740431/2203797932.py:1: DtypeWarning: Columns (7,9,10,13,20,25,30,34,42,44,56,58,68,78,80,114,115,138,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nhtsa_cleaned = pd.read_csv(data_path / \"NHTSA_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "nhtsa_cleaned = pd.read_csv(data_path / \"NHTSA_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2ac65058-d2a4-4fc6-9873-8a7f48adc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the cleaned file\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned[[\"VIN\", \"Manufacturer\", \"Model\", \"ModelYear\", \"FuelTypePrimary\", \"ElectrificationLevel\"]]\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned_simple.rename(columns = {\"VIN\":\"vin_corrected\",\n",
    "                                                              \"Manufacturer\" : \"Manufacturer Name\",\n",
    "                                                              \"ModelYear\" : \"Model Year\",\n",
    "                                                              \"FuelTypePrimary\" : \"Fuel Type - Primary\",\n",
    "                                                              \"ElectrificationLevel\" : \"Electrification Level\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7e79007-8494-4d9e-8645-645e668b7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "queried_vins = pd.read_csv(data_path / \"matching_list_197200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9aa08c54-9159-458b-9780-3ebc873b0ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Manufacturer Name', 'Model', 'Model Year', 'Fuel Type - Primary',\n",
       "       'Electrification Level', 'vin_corrected'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queried_vins.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f068c318-05a4-4d50-9b5e-473880161bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vins = pd.concat([nhtsa_cleaned_simple, queried_vins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "294c9d2e-d647-4a23-b72c-a571b0554565",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vins = all_vins.loc[all_vins[\"Manufacturer Name\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3344afe-e029-47ad-a052-e961b4a1178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vins.to_csv(data_path / \"all_vins_nhtsa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d95ecb-c79d-48ef-935c-64b5392a1a72",
   "metadata": {},
   "source": [
    "# Run again with new matching list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16848d6f-3f1c-4f8e-accd-0e2b78b6b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", header=0, chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec367834-bbb1-41d6-8ce7-e77f545d7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_run = 5900\n",
    "vm = VINMatcher(all_vins)\n",
    "cp = ChunkProcessor(chunks, processed_chunks_save_path, number_to_run, vm, \"zip\", \"vehicle_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077aff8-d8da-4c50-bdbf-bc83d27d20a0",
   "metadata": {},
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e760c-89de-4d5f-bffd-ba9504463f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c2cd3a4c-65f0-4cd2-a29d-002b7776dba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/gpfs/gibbs/project/gillingham/rrn22/processed_chunks_new_save/matched_chunk_number_3099.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matched \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_chunks_save_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatched_chunk_number_3099.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m unmatched \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(processed_chunks_save_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munmatched_chunk_number_3099.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m join \u001b[38;5;241m=\u001b[39m matched\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/gibbs/project/gillingham/rrn22/processed_chunks_new_save/matched_chunk_number_3099.csv'"
     ]
    }
   ],
   "source": [
    "matched = pd.read_csv(processed_chunks_save_path / \"matched_chunk_number_3099.csv\")\n",
    "unmatched = pd.read_csv(processed_chunks_save_path / \"unmatched_chunk_number_3099.csv\")\n",
    "\n",
    "join = matched.copy()\n",
    "join.loc[join[\"vin_corrected\"].isna()] = unmatched.loc[join[\"vin_corrected\"].isna()]\n",
    "join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ab80ca36-e43d-47e4-8e46-0e52214a4875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>record_from</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>vehicle_year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "      <th>vehicle_class</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>zip_corrected</th>\n",
       "      <th>vin_corrected</th>\n",
       "      <th>Manufacturer Name</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Fuel Type - Primary</th>\n",
       "      <th>Electrification Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>110_Plainville_MVData_2019.csv</td>\n",
       "      <td>VELOCCIA ELIZABETH H</td>\n",
       "      <td>140 PICKNEY AVE</td>\n",
       "      <td>PLAINVILLE</td>\n",
       "      <td>CT</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>3.2 TL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19UUA66276A056130</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>19UUA662*6A</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>TL</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>110_Plainville_MVData_2019.csv</td>\n",
       "      <td>VELOCCIA ELIZABETH H</td>\n",
       "      <td>140 PICKNEY AVE</td>\n",
       "      <td>PLAINVILLE</td>\n",
       "      <td>CT</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INFIN</td>\n",
       "      <td>Q50/PREM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>JN1BV7AR6FM400066</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>JN1BV7AR*FM</td>\n",
       "      <td>NISSAN MOTOR COMPANY, LTD</td>\n",
       "      <td>Q50</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>110_Plainville_MVData_2019.csv</td>\n",
       "      <td>VELOCCIA VINCENT</td>\n",
       "      <td>140 PICKNEY AVE</td>\n",
       "      <td>PLAINVILLE</td>\n",
       "      <td>CT</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KAWAS</td>\n",
       "      <td>VN800</td>\n",
       "      <td>12.0</td>\n",
       "      <td>JKBVNCA17SA009586</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>JKBVNCA1*SA</td>\n",
       "      <td>KAWASAKI MOTORS, LTD</td>\n",
       "      <td>Vulcan 800</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>110_Plainville_MVData_2019.csv</td>\n",
       "      <td>VELOCCIA VINCENZO</td>\n",
       "      <td>140 PICKNEY AVE</td>\n",
       "      <td>PLAINVILLE</td>\n",
       "      <td>CT</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOYOT</td>\n",
       "      <td>TACOMA A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5TFUU4EN0CX036008</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>5TFUU4EN*CX</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>110_Plainville_MVData_2019.csv</td>\n",
       "      <td>VELODOTA DONALD E JR</td>\n",
       "      <td>121 CAMP ST</td>\n",
       "      <td>PLAINVILLE</td>\n",
       "      <td>CT</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEVR</td>\n",
       "      <td>CAMARO S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2G1FK1EJ7A9148805</td>\n",
       "      <td>6062.0</td>\n",
       "      <td>2G1FK1EJ*A9</td>\n",
       "      <td>GENERAL MOTORS LLC</td>\n",
       "      <td>Camaro</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181995</th>\n",
       "      <td>995</td>\n",
       "      <td>128_Simsbury_MVData_2019.csv</td>\n",
       "      <td>KILLE BENJAMIN D</td>\n",
       "      <td>94 GREAT POND RD</td>\n",
       "      <td>SIMSBURY</td>\n",
       "      <td>CT</td>\n",
       "      <td>06070</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>CHEVR</td>\n",
       "      <td>SILVERAD</td>\n",
       "      <td>3</td>\n",
       "      <td>1GCVKREC2FZ403022</td>\n",
       "      <td>6070.0</td>\n",
       "      <td>1GCVKREC*FZ</td>\n",
       "      <td>GENERAL MOTORS LLC</td>\n",
       "      <td>Silverado</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181996</th>\n",
       "      <td>996</td>\n",
       "      <td>128_Simsbury_MVData_2019.csv</td>\n",
       "      <td>KIM CHI H</td>\n",
       "      <td>12 JOSHUA DR</td>\n",
       "      <td>WEST SIMSBURY</td>\n",
       "      <td>CT</td>\n",
       "      <td>06092</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>PORSC</td>\n",
       "      <td>MACAN GT</td>\n",
       "      <td>1</td>\n",
       "      <td>WP1AG2A56HLB54792</td>\n",
       "      <td>6092.0</td>\n",
       "      <td>WP1AG2A5*HL</td>\n",
       "      <td>PORSCHE</td>\n",
       "      <td>Macan</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181997</th>\n",
       "      <td>997</td>\n",
       "      <td>128_Simsbury_MVData_2019.csv</td>\n",
       "      <td>KIM EDWARD C</td>\n",
       "      <td>145 COOPER AVE UNIT 8</td>\n",
       "      <td>WEATOGUE</td>\n",
       "      <td>CT</td>\n",
       "      <td>06089</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>MERCE</td>\n",
       "      <td>GLK350 4</td>\n",
       "      <td>1</td>\n",
       "      <td>WDCGG8JBXDG114944</td>\n",
       "      <td>6089.0</td>\n",
       "      <td>WDCGG8JB*DG</td>\n",
       "      <td>MERCEDES-BENZ</td>\n",
       "      <td>GLK Class</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181998</th>\n",
       "      <td>998</td>\n",
       "      <td>128_Simsbury_MVData_2019.csv</td>\n",
       "      <td>KIM JIHYUN</td>\n",
       "      <td>712D HOPMEADOW ST</td>\n",
       "      <td>SIMSBURY</td>\n",
       "      <td>CT</td>\n",
       "      <td>06070</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>NISSA</td>\n",
       "      <td>ROGUE S/</td>\n",
       "      <td>1</td>\n",
       "      <td>JN8AS5MV7AW109608</td>\n",
       "      <td>6070.0</td>\n",
       "      <td>JN8AS5MV*AW</td>\n",
       "      <td>NISSAN</td>\n",
       "      <td>Rogue</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181999</th>\n",
       "      <td>999</td>\n",
       "      <td>128_Simsbury_MVData_2019.csv</td>\n",
       "      <td>KIM JIHYUN</td>\n",
       "      <td>712D HOPMEADOW ST</td>\n",
       "      <td>SIMSBURY</td>\n",
       "      <td>CT</td>\n",
       "      <td>06070</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>ODYSSEY</td>\n",
       "      <td>1</td>\n",
       "      <td>5FNRL6H99JB007570</td>\n",
       "      <td>6070.0</td>\n",
       "      <td>5FNRL6H9*JB</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>Odyssey</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182000 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                     record_from  \\\n",
       "0                0  110_Plainville_MVData_2019.csv   \n",
       "1                1  110_Plainville_MVData_2019.csv   \n",
       "2                2  110_Plainville_MVData_2019.csv   \n",
       "3                3  110_Plainville_MVData_2019.csv   \n",
       "4                4  110_Plainville_MVData_2019.csv   \n",
       "...            ...                             ...   \n",
       "181995         995    128_Simsbury_MVData_2019.csv   \n",
       "181996         996    128_Simsbury_MVData_2019.csv   \n",
       "181997         997    128_Simsbury_MVData_2019.csv   \n",
       "181998         998    128_Simsbury_MVData_2019.csv   \n",
       "181999         999    128_Simsbury_MVData_2019.csv   \n",
       "\n",
       "                                            name  \\\n",
       "0       VELOCCIA ELIZABETH H                       \n",
       "1       VELOCCIA ELIZABETH H                       \n",
       "2       VELOCCIA VINCENT                           \n",
       "3       VELOCCIA VINCENZO                          \n",
       "4       VELODOTA DONALD E JR                       \n",
       "...                                          ...   \n",
       "181995  KILLE BENJAMIN D                           \n",
       "181996  KIM CHI H                                  \n",
       "181997  KIM EDWARD C                               \n",
       "181998  KIM JIHYUN                                 \n",
       "181999  KIM JIHYUN                                 \n",
       "\n",
       "                                street                            city state  \\\n",
       "0       140 PICKNEY AVE                 PLAINVILLE                        CT   \n",
       "1       140 PICKNEY AVE                 PLAINVILLE                        CT   \n",
       "2       140 PICKNEY AVE                 PLAINVILLE                        CT   \n",
       "3       140 PICKNEY AVE                 PLAINVILLE                        CT   \n",
       "4       121 CAMP ST                     PLAINVILLE                        CT   \n",
       "...                                ...                             ...   ...   \n",
       "181995  94 GREAT POND RD                SIMSBURY                          CT   \n",
       "181996  12 JOSHUA DR                    WEST SIMSBURY                     CT   \n",
       "181997  145 COOPER AVE UNIT 8           WEATOGUE                          CT   \n",
       "181998  712D HOPMEADOW ST               SIMSBURY                          CT   \n",
       "181999  712D HOPMEADOW ST               SIMSBURY                          CT   \n",
       "\n",
       "           zip  vehicle_year vehicle_make vehicle_model vehicle_class  \\\n",
       "0       6062.0           NaN        ACURA      3.2 TL             1.0   \n",
       "1       6062.0           NaN        INFIN      Q50/PREM           1.0   \n",
       "2       6062.0           NaN        KAWAS      VN800             12.0   \n",
       "3       6062.0           NaN        TOYOT      TACOMA A           3.0   \n",
       "4       6062.0           NaN        CHEVR      CAMARO S           1.0   \n",
       "...        ...           ...          ...           ...           ...   \n",
       "181995   06070        2015.0        CHEVR      SILVERAD             3   \n",
       "181996   06092        2017.0        PORSC      MACAN GT             1   \n",
       "181997   06089        2013.0        MERCE      GLK350 4             1   \n",
       "181998   06070        2010.0        NISSA      ROGUE S/             1   \n",
       "181999   06070        2017.0        HONDA      ODYSSEY              1   \n",
       "\n",
       "               vehicle_id  zip_corrected vin_corrected  \\\n",
       "0       19UUA66276A056130         6062.0   19UUA662*6A   \n",
       "1       JN1BV7AR6FM400066         6062.0   JN1BV7AR*FM   \n",
       "2       JKBVNCA17SA009586         6062.0   JKBVNCA1*SA   \n",
       "3       5TFUU4EN0CX036008         6062.0   5TFUU4EN*CX   \n",
       "4       2G1FK1EJ7A9148805         6062.0   2G1FK1EJ*A9   \n",
       "...                   ...            ...           ...   \n",
       "181995  1GCVKREC2FZ403022         6070.0   1GCVKREC*FZ   \n",
       "181996  WP1AG2A56HLB54792         6092.0   WP1AG2A5*HL   \n",
       "181997  WDCGG8JBXDG114944         6089.0   WDCGG8JB*DG   \n",
       "181998  JN8AS5MV7AW109608         6070.0   JN8AS5MV*AW   \n",
       "181999  5FNRL6H99JB007570         6070.0   5FNRL6H9*JB   \n",
       "\n",
       "                Manufacturer Name       Model  Model Year Fuel Type - Primary  \\\n",
       "0                           HONDA          TL      2006.0            Gasoline   \n",
       "1       NISSAN MOTOR COMPANY, LTD         Q50      2015.0            Gasoline   \n",
       "2            KAWASAKI MOTORS, LTD  Vulcan 800      1995.0                 NaN   \n",
       "3                          TOYOTA      Tacoma      2012.0            Gasoline   \n",
       "4              GENERAL MOTORS LLC      Camaro      2010.0            Gasoline   \n",
       "...                           ...         ...         ...                 ...   \n",
       "181995         GENERAL MOTORS LLC   Silverado      2015.0            Gasoline   \n",
       "181996                    PORSCHE       Macan      2017.0            Gasoline   \n",
       "181997              MERCEDES-BENZ   GLK Class      2013.0            Gasoline   \n",
       "181998                     NISSAN       Rogue      2010.0            Gasoline   \n",
       "181999                      HONDA     Odyssey      2018.0            Gasoline   \n",
       "\n",
       "       Electrification Level  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                        NaN  \n",
       "4                        NaN  \n",
       "...                      ...  \n",
       "181995                   NaN  \n",
       "181996                   NaN  \n",
       "181997                   NaN  \n",
       "181998                   NaN  \n",
       "181999                   NaN  \n",
       "\n",
       "[182000 rows x 19 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
