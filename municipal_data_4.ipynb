{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3778eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "import usaddress\n",
    "\n",
    "from difflib import get_close_matches as clmatch\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely import Point\n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "\n",
    "path = pathlib.Path().resolve()\n",
    "\n",
    "data_path = path.parent / \"Dropbox\" / \"2019 MV Data by Town\" / \"Vehicles_2022\" / \"Compiled\"\n",
    "\n",
    "processed_chunks_path = path.parent / \"Dropbox\" / \"DOT_Tobin_Collaboration\" / \"data\" / \"processed_chunks_2\"\n",
    "\n",
    "matching_list_path = path.parent / \"Dropbox\" / \"DOT_Tobin_Collaboration\" / \"data\" \n",
    "\n",
    "logging_path = path.parent / \"Dropbox\" / \"DOT_Tobin_Collaboration\" / \"data\" \n",
    "\n",
    "raw_data = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", chunksize = 1000)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "11f6ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = logging_path / \"municipal_data_prep_1200_test.log\", level = logging.INFO,  format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb256647",
   "metadata": {},
   "source": [
    "# Full processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0381bf6c",
   "metadata": {},
   "source": [
    "## Prepare data and import matching sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c37a08",
   "metadata": {},
   "source": [
    "## Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8ad7cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkProcessor():\n",
    "    def __init__(self, chunks, \n",
    "                 processed_chunk_path, \n",
    "                 number_to_run, \n",
    "                 vin_matcher, zip_column_name, vin_column_name):\n",
    "        self.chunks = chunks\n",
    "        self.processed_chunks = pd.DataFrame([])\n",
    "        self.processed_chunks_path = processed_chunk_path\n",
    "        \n",
    "        self.matched_chunks = pd.DataFrame([])\n",
    "        self.unmatched_chunks = pd.DataFrame([])\n",
    "        self.tally = pd.DataFrame([])\n",
    "        \n",
    "        self.number_to_run = number_to_run\n",
    "        self.chunk_number = 0\n",
    "        self.vin_matcher = vin_matcher\n",
    "        self.zip_column_name = zip_column_name\n",
    "        self.vin_column_name = vin_column_name\n",
    "        \n",
    "    def run(self):\n",
    "        for chunk in self.chunks:\n",
    "            if self.chunk_number < self.number_to_run:\n",
    "                self.process_chunk(chunk)\n",
    "            else:\n",
    "                logging.info(f\"Concluding on chunk number {self.chunk_number}\")\n",
    "                break\n",
    "    \n",
    "    def process_chunk(self, chunk):\n",
    "        try:\n",
    "            # Display Progress\n",
    "            if self.chunk_number % 10 == 0:\n",
    "                print(f\"Currently processing chunk number {self.chunk_number}\")\n",
    "            \n",
    "            # Log progress\n",
    "            logging.info(f\"Chunk Number {self.chunk_number}: commencing processing\")\n",
    "            logging.info(f\"Chunk Number {self.chunk_number}: chunk length is {len(chunk)}\")\n",
    "        \n",
    "            # Reduce reduce the number of columns\n",
    "            chunk_simplified = chunk[['record_from', 'name', 'street', 'city', \n",
    "                                  'state', 'zip', 'vehicle_year', 'vehicle_make', 'vehicle_model',\n",
    "                                  'vehicle_class', 'vehicle_id']].reset_index(drop = True)\n",
    "        \n",
    "            # Get the corrected zip codes, and add them as a column\n",
    "            corrected_zip_codes = self.get_valid_zips(chunk_simplified, self.zip_column_name).reset_index(drop = True)\n",
    "            chunk_simplified = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\n",
    "\n",
    "            # Get two dataframes: matched vins, and unmatched vins, and a tally\n",
    "            # Does NOT change the index \n",
    "            matched_vins, unmatched_vins, tally = self.vin_matcher.match_vins_simple(chunk_simplified,\n",
    "                                                                                    self.vin_column_name,\n",
    "                                                                                   self.chunk_number)\n",
    "            # Join to the original data, keeping original indices\n",
    "            matched_vins = chunk_simplified.join(matched_vins)\n",
    "            unmatched_vins = chunk_simplified.join(unmatched_vins)\n",
    "            \n",
    "            # Confirm tally. This number should be the chunk length\n",
    "            tally[\"confirm_matched\"] = len(matched_vins)\n",
    "            tally[\"confirm_unmatched\"] = len(unmatched_vins)\n",
    "            \n",
    "            # Aggregate\n",
    "            self.aggregate_save_matched_unmatched(matched_vins,\n",
    "                                                 unmatched_vins,\n",
    "                                                 tally)            \n",
    "            \n",
    "            # matched_vins = self.vin_matcher.match_vins(chunk, self.vin_column_name).reset_index(drop=True)\n",
    "\n",
    "            # chunk_processed = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\n",
    "            # chunk_processed = chunk_processed.join(matched_vins)\n",
    "            \n",
    "            # Aggregate and save\n",
    "            # self.aggregate_save_chunk(chunk_processed)\n",
    "\n",
    "            # Update chunk number\n",
    "            self.chunk_number +=1\n",
    "            \n",
    "            # return chunk_processed\n",
    "        except Exception as e:\n",
    "            # THIS IS THE CAUSE - if something falls through, we skip the rest of the chunk. \n",
    "            logging.error(f\"Error encountered on chunk {self.chunk_number}, this means the last chunk to be run was chunk {self.chunk_number-1}\")\n",
    "            logging.error(e)\n",
    "            print(f\"Error encountered on chunk {self.chunk_number}, this means the last chunk to be run was chunk {self.chunk_number-1}\")\n",
    "            print(e)\n",
    "            self.chunk_number +=1\n",
    "        \n",
    "    def aggregate_save_chunk(self, processed_chunk):        \n",
    "        # Add it to the master DF\n",
    "        self.processed_chunks = pd.concat([self.processed_chunks, processed_chunk])\n",
    "        \n",
    "        if ((self.chunk_number % 10)+1 ==10) & (self.chunk_number>0):\n",
    "            logging.info(f\"Saving aggregated chunks numbers {self.chunk_number-10} - {self.chunk_number}\")\n",
    "            dt_string = now.strftime(\"%d%m%y_%H%M\")\n",
    "            self.processed_chunks.to_csv(self.processed_chunks_path / f\"chunk_number_{self.chunk_number}_{dt_string}.csv\")\n",
    "        \n",
    "        # Once saved, reset the DF\n",
    "        self.processed_chunks = pd.DataFrame([])\n",
    "        \n",
    "    def aggregate_save_matched_unmatched(self, matched_chunk, unmatched_chunk, tally): \n",
    "        \"\"\"Input: A DataFrame of data for matched vins, a DF for unmatched VINS, and a tally.\n",
    "        Aggregates this data over 10 chunks, then saves a file\n",
    "        \"\"\"\n",
    "        # Append data to three separate master DFs\n",
    "        self.matched_chunks = pd.concat([self.matched_chunks, matched_chunk])\n",
    "        self.unmatched_chunks = pd.concat([self.unmatched_chunks, unmatched_chunk])\n",
    "        self.tally = pd.concat([self.tally, tally])\n",
    "        \n",
    "        # Every 10th chunk, save\n",
    "        if ((self.chunk_number % 100)+1 ==100) & (self.chunk_number>0):\n",
    "            # Log\n",
    "            logging.info(f\"Saving chunk {self.chunk_number}\")\n",
    "            \n",
    "            # Save the three DataFrames\n",
    "            self.matched_chunks.to_csv(self.processed_chunks_path / f\"matched_chunk_number_{self.chunk_number}.csv\")\n",
    "            self.unmatched_chunks.to_csv(self.processed_chunks_path / f\"unmatched_chunk_number_{self.chunk_number}.csv\")\n",
    "            self.tally.to_csv(self.processed_chunks_path / f\"tally_chunk_number_{self.chunk_number}.csv\")\n",
    "\n",
    "            # Once saved, reset the DF\n",
    "            self.matched_chunks = pd.DataFrame([])\n",
    "            self.unmatched_chunks = pd.DataFrame([])\n",
    "            self.tally = pd.DataFrame([])\n",
    "            \n",
    "            \n",
    "    def check_valid_zip(self, zip_code):\n",
    "        zip_str = str(zip_code)\n",
    "        split_zip = re.split(\"-\", zip_str)\n",
    "        if len(split_zip) == 2:\n",
    "            if self.check_valid_zip(split_zip[0]) & self.check_valid_zip(split_zip[1]):\n",
    "                return 2\n",
    "            elif self.check_valid_zip(split_zip[0]):\n",
    "                return 3\n",
    "            else:\n",
    "                return 0\n",
    "        elif len(split_zip) == 1:\n",
    "            # MUST ADDRESS STARTING \"Os\"\n",
    "            # starting_o = re.match('^O', split_zip[0])\n",
    "            matched = re.match(\"^\\s*[0-9]*[0-9]{4}\\.?0?\\s*$\", split_zip[0])\n",
    "            if matched:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def get_valid_zips(self, zip_df, zip_column_name):\n",
    "        \"\"\"\n",
    "        Inputs: A DataFrame with a column called \"zip\"\n",
    "        Returns: A DataFrame of the same length as the input, with three columns: zip, zip_valid_code, and zip_corrected\n",
    "        \"\"\"\n",
    "        # Prepare the list to be used\n",
    "        zip_list = zip_df[[zip_column_name]].rename(columns = {zip_column_name : \"zip\"}).reset_index(drop = True)\n",
    "\n",
    "        # Get validity code\n",
    "        zip_list.loc[:, \"zip_valid_code\"] = zip_list.loc[:, \"zip\"].apply(lambda x: self.check_valid_zip(x))\n",
    "\n",
    "        # Get indices\n",
    "        correct_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==1].index\n",
    "        invalid_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==0].index\n",
    "        two_part_zips_indices = zip_list[zip_list[\"zip_valid_code\"]>1].index\n",
    "\n",
    "        zip_list.loc[correct_zips_indices, \"zip_corrected\"] = zip_list.loc[correct_zips_indices, \"zip\"]\n",
    "        zip_list.loc[invalid_zips_indices, \"zip_corrected\"] = np.NaN\n",
    "        zip_list.loc[two_part_zips_indices, \"zip_corrected\"] = zip_list.loc[two_part_zips_indices, \"zip\"].astype(str).str[0:5]\n",
    "\n",
    "        return zip_list[[\"zip_corrected\"]]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "05385113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VINMatcher():\n",
    "    def __init__(self, initial_matching_list):\n",
    "        self.matching_list = initial_matching_list.drop_duplicates(\"vin_corrected\")\n",
    "        self.queries = 0\n",
    "        \n",
    "    def valid_vins(self, vins_to_correct_df, vin_column):\n",
    "        \"\"\"\n",
    "        Input: A DataFrame containing a \"vehicle_id\" column of VINs to be corrected\n",
    "        Output: A DataFrame with two columns: \"vehicle_id\" and \"vin_corrected\"\n",
    "        \"\"\"\n",
    "        # Set up\n",
    "        vin_list = vins_to_correct_df        \n",
    "\n",
    "        # Check all alphanumerics\n",
    "        # THIS IS WHERE THE ERROR IS - SOMETIMES HIT A BLANK VIN AND THEREFORE THIS FAILS\n",
    "        # AS IT IS NOT A STRING\n",
    "        # I HAVE APPLIED A STRING OPERATION WITHOUT KNOWING IF STRING FIRST\n",
    "        not_na_mask = vin_list[vin_column].notna()\n",
    "        na_mask = vin_list[vin_column].isna()\n",
    "        \n",
    "        vin_list.loc[not_na_mask, \"vin_alnum_check\"] = vin_list.loc[not_na_mask, vin_column].astype(str).str.strip().str.isalnum()\n",
    "        vin_list.loc[not_na_mask, \"vin_len_check\"] = vin_list.loc[not_na_mask, vin_column].astype(str).str.len() >= 11\n",
    "        \n",
    "        vin_list.loc[na_mask, \"vin_alnum_check\"] = False\n",
    "        vin_list.loc[na_mask, \"vin_len_check\"] = False\n",
    "        \n",
    "        vin_list[\"vin_check\"] = vin_list[\"vin_alnum_check\"] & vin_list[\"vin_len_check\"]\n",
    "\n",
    "        # Create 11-long vins  \n",
    "        vin_list[\"prepared_vins\"] = np.NaN\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"prepared_vins\"] = vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index,vin_column].astype(str).str[0:8]+\"*\"+vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index,vin_column].astype(str).str[9:11]\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==False].index, \"vin_corrected\"] = np.NaN\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"vin_corrected\"] = vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"prepared_vins\"]\n",
    "\n",
    "        # Clean up\n",
    "        vin_list = vin_list.drop(\"prepared_vins\", axis = 1)\n",
    "        vin_list = vin_list[[\"vin_corrected\"]]\n",
    "\n",
    "        # Return\n",
    "        return vin_list.reset_index(drop = True)\n",
    "    \n",
    "    def match_vins_simple(self, df, vin_column, chunk_number):\n",
    "        # Get a list of valid VINs\n",
    "        valid_vin_list = self.valid_vins(df[[vin_column]], vin_column)\n",
    "        \n",
    "        # Attempt a match\n",
    "        match = valid_vin_list.merge(self.matching_list,\n",
    "                                     left_on = \"vin_corrected\",\n",
    "                                     right_on = \"vin_corrected\",\n",
    "                                     how = 'left')\n",
    "        \n",
    "        # Get rows of DF where VINS matched\n",
    "        df_vins_matched = match.loc[match[\"Manufacturer Name\"].notna(), :]\n",
    "        df_vins_unmatched = match.loc[match[\"Manufacturer Name\"].isna(), :]\n",
    "        \n",
    "        # Get length\n",
    "        len_matched = len(df_vins_matched)\n",
    "        len_unmatched = len(df_vins_unmatched)\n",
    "        \n",
    "        # Create df\n",
    "        tally_dict = {\"Chunk Number\": [chunk_number],\n",
    "                      \"Matched\" : [len_matched],\n",
    "                      \"Unmatched\" : [len_unmatched]}\n",
    "        \n",
    "        match_unmatched_tally = pd.DataFrame(tally_dict)\n",
    "        \n",
    "        return [df_vins_matched, df_vins_unmatched, match_unmatched_tally]\n",
    "        \n",
    "    def match_vins(self, df, vin_column):\n",
    "        \"\"\"\n",
    "        Input: A df containing vin columns, that are then corrected, and matched\n",
    "        Returns: matched vins, updated matching list    \n",
    "        \"\"\"\n",
    "        # Get a list of valid VINs\n",
    "        valid_vin_list = self.valid_vins(df[[vin_column]], vin_column)\n",
    "        \n",
    "        # Attempt a match\n",
    "        match = valid_vin_list.merge(self.matching_list,\n",
    "                                     left_on = \"vin_corrected\",\n",
    "                                     right_on = \"vin_corrected\",\n",
    "                                     how = 'left')\n",
    "                                      \n",
    "                                      \n",
    "                                      \n",
    "        # Get unique unmatched vins\n",
    "        unmatched_vins = list(match[match[\"Manufacturer Name\"].isna()][\"vin_corrected\"].unique())\n",
    "\n",
    "        # Print how many\n",
    "        logging.info(f\"VIN matching: a total of {len(unmatched_vins)} VINs were not matched\")\n",
    "\n",
    "        # Variables to download\n",
    "        variables = [\"Manufacturer Name\", \"Model\", \"Model Year\", \"Fuel Type - Primary\", \"Electrification Level\"]\n",
    "        \n",
    "        # Go get them\n",
    "        for vin in tqdm(unmatched_vins):\n",
    "            try:\n",
    "                # Try to fetch the unmatched vin\n",
    "                resp_df = self.fetch_unmatched_vin(vin).reset_index(drop=True)\n",
    "                for variable in variables:\n",
    "                    match.loc[match[match[\"vin_corrected\"]==vin].index, match.columns.isin([variable])] = resp_df[variable][0]\n",
    "                \n",
    "            except BaseException as e:\n",
    "                logging.info(e)\n",
    "                pass\n",
    "\n",
    "        remaining_unmatched = list(match[match[\"Manufacturer Name\"].isna()][\"vin_corrected\"].unique())\n",
    "        \n",
    "        logging.info(f\"VIN Matching: this number of unmatched VINs was reduced by {len(unmatched_vins) - len(remaining_unmatched)}\")\n",
    "        logging.info(f\"VIN Matching: remaining unmatched VINs is {len(remaining_unmatched)}\")\n",
    "\n",
    "        return match\n",
    "    \n",
    "    def fetch_unmatched_vin(self, unmatched_vin):\n",
    "        \"\"\"\n",
    "        Input: An unmatched, but corrected VIN\n",
    "        Output: A matched VIN or NA\n",
    "        \n",
    "        \"\"\"\n",
    "        # Increment the number of times queried\n",
    "        self.queries +=1\n",
    "        \n",
    "        variables = [\"Manufacturer Name\", \"Model\", \"Model Year\", \"Fuel Type - Primary\", \"Electrification Level\"]\n",
    "        \n",
    "        url = (f\"https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVin/{unmatched_vin.strip()}?format=csv\")\n",
    "\n",
    "        # Download response\n",
    "        resp_df = pd.read_csv(url)\n",
    "\n",
    "        # Extract needed\n",
    "        resp_df = resp_df.loc[resp_df[\"variable\"].isin(variables), [\"variable\", \"value\"]].T\n",
    "        resp_df.columns = resp_df.iloc[0]\n",
    "        resp_df = resp_df.drop(\"variable\", axis = 0)\n",
    "        resp_df[\"vin_corrected\"] = unmatched_vin\n",
    "        valid_response = not(resp_df[\"Fuel Type - Primary\"].isna()[0])\n",
    "        \n",
    "        # Update the matching list\n",
    "        self.matching_list = pd.concat([self.matching_list, resp_df]).reset_index(drop = True)\n",
    "        \n",
    "        # If the number of queries is an increment of 100, save the matching list        \n",
    "        if self.queries % 10 == 0:\n",
    "            logging.info(f\"Saving matching list after {self.queries} queries\")\n",
    "            self.matching_list.to_csv(matching_list_path / \"matching_list.csv\", index = False)\n",
    "            self.matching_list = self.matching_list.drop_duplicates(\"vin_corrected\")\n",
    "        \n",
    "        return resp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284582b",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a8f608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks_path\n",
    "number_to_run = 5900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8ccdbc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajan\\AppData\\Local\\Temp\\ipykernel_14972\\1811061293.py:2: DtypeWarning: Columns (7,9,10,13,20,25,30,34,42,44,56,58,68,78,80,114,115,138,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nhtsa_cleaned = pd.read_csv(path / \"ignored-data\" / \"NHTSA_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load NHTSA data\n",
    "nhtsa_cleaned = pd.read_csv(path / \"ignored-data\" / \"NHTSA_cleaned.csv\")\n",
    "\n",
    "# Simplify the cleaned file\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned[[\"VIN\", \"Manufacturer\", \"Model\", \"ModelYear\", \"FuelTypePrimary\", \"ElectrificationLevel\"]]\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned_simple.rename(columns = {\"VIN\":\"vin_corrected\",\n",
    "                                                              \"Manufacturer\" : \"Manufacturer Name\",\n",
    "                                                              \"ModelYear\" : \"Model Year\",\n",
    "                                                              \"FuelTypePrimary\" : \"Fuel Type - Primary\",\n",
    "                                                              \"ElectrificationLevel\" : \"Electrification Level\"})\n",
    "matching_list = pd.read_csv(matching_list_path / \"matching_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7ffd3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", header=0, chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a08e9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = VINMatcher(nhtsa_cleaned_simple)\n",
    "cp = ChunkProcessor(chunks, processed_chunks_path, number_to_run, vm, \"zip\", \"vehicle_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "462cc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_2 = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", header=0, chunksize = 1000, skiprows=range(1,2930000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "111b1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_2 = VINMatcher(nhtsa_cleaned_simple)\n",
    "cp_2 = ChunkProcessor(chunks_2, processed_chunks_path, number_to_run, vm_2, \"zip\", \"vehicle_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3d4e3665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 0\n",
      "Currently processing chunk number 10\n",
      "Currently processing chunk number 20\n",
      "Currently processing chunk number 30\n",
      "Currently processing chunk number 40\n",
      "Currently processing chunk number 50\n",
      "Currently processing chunk number 60\n",
      "Currently processing chunk number 70\n",
      "Currently processing chunk number 80\n",
      "Currently processing chunk number 90\n",
      "Currently processing chunk number 100\n",
      "Currently processing chunk number 110\n",
      "Currently processing chunk number 120\n",
      "Currently processing chunk number 130\n",
      "Currently processing chunk number 140\n",
      "Currently processing chunk number 150\n",
      "Currently processing chunk number 160\n",
      "Currently processing chunk number 170\n",
      "Currently processing chunk number 180\n",
      "Currently processing chunk number 190\n",
      "Currently processing chunk number 200\n",
      "Currently processing chunk number 210\n",
      "Currently processing chunk number 220\n",
      "Currently processing chunk number 230\n",
      "Currently processing chunk number 240\n",
      "Currently processing chunk number 250\n",
      "Currently processing chunk number 260\n",
      "Currently processing chunk number 270\n",
      "Currently processing chunk number 280\n",
      "Currently processing chunk number 290\n",
      "Currently processing chunk number 300\n",
      "Currently processing chunk number 310\n",
      "Currently processing chunk number 320\n",
      "Currently processing chunk number 330\n",
      "Currently processing chunk number 340\n",
      "Currently processing chunk number 350\n",
      "Currently processing chunk number 360\n",
      "Currently processing chunk number 370\n",
      "Currently processing chunk number 380\n",
      "Currently processing chunk number 390\n",
      "Currently processing chunk number 400\n",
      "Currently processing chunk number 410\n",
      "Currently processing chunk number 420\n",
      "Currently processing chunk number 430\n",
      "Currently processing chunk number 440\n",
      "Currently processing chunk number 450\n",
      "Currently processing chunk number 460\n",
      "Currently processing chunk number 470\n",
      "Currently processing chunk number 480\n",
      "Currently processing chunk number 490\n",
      "Currently processing chunk number 500\n",
      "Currently processing chunk number 510\n",
      "Currently processing chunk number 520\n",
      "Currently processing chunk number 530\n",
      "Currently processing chunk number 540\n",
      "Currently processing chunk number 550\n",
      "Currently processing chunk number 560\n",
      "Currently processing chunk number 570\n",
      "Currently processing chunk number 580\n",
      "Currently processing chunk number 590\n",
      "Currently processing chunk number 600\n",
      "Currently processing chunk number 610\n",
      "Currently processing chunk number 620\n",
      "Currently processing chunk number 630\n",
      "Currently processing chunk number 640\n",
      "Currently processing chunk number 650\n",
      "Currently processing chunk number 660\n",
      "Currently processing chunk number 670\n",
      "Currently processing chunk number 680\n",
      "Currently processing chunk number 690\n",
      "Currently processing chunk number 700\n",
      "Currently processing chunk number 710\n",
      "Currently processing chunk number 720\n",
      "Currently processing chunk number 730\n",
      "Currently processing chunk number 740\n",
      "Currently processing chunk number 750\n",
      "Currently processing chunk number 760\n",
      "Currently processing chunk number 770\n",
      "Currently processing chunk number 780\n",
      "Currently processing chunk number 790\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [249]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcp_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [224]\u001b[0m, in \u001b[0;36mChunkProcessor.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_to_run:\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcluding on chunk number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [224]\u001b[0m, in \u001b[0;36mChunkProcessor.process_chunk\u001b[1;34m(self, chunk)\u001b[0m\n\u001b[0;32m     58\u001b[0m tally[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfirm_unmatched\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unmatched_vins)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Aggregate\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_save_matched_unmatched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatched_vins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43munmatched_vins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtally\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# matched_vins = self.vin_matcher.match_vins(chunk, self.vin_column_name).reset_index(drop=True)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# chunk_processed = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Update chunk number\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [224]\u001b[0m, in \u001b[0;36mChunkProcessor.aggregate_save_matched_unmatched\u001b[1;34m(self, matched_chunk, unmatched_chunk, tally)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Save the three DataFrames\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatched_chunks\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_chunks_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatched_chunk_number_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munmatched_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_chunks_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munmatched_chunk_number_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_number\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtally\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_chunks_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtally_chunk_number_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Once saved, reset the DF\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3554\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3556\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:315\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    312\u001b[0m data \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39miget_values(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mitems))]\n\u001b[0;32m    314\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 315\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\writers.pyx:55\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cp_2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6291e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 0\n",
      "Currently processing chunk number 10\n",
      "Currently processing chunk number 20\n",
      "Currently processing chunk number 30\n",
      "Currently processing chunk number 40\n",
      "Currently processing chunk number 50\n",
      "Currently processing chunk number 60\n",
      "Currently processing chunk number 70\n",
      "Currently processing chunk number 80\n",
      "Currently processing chunk number 90\n",
      "Currently processing chunk number 100\n",
      "Currently processing chunk number 110\n",
      "Currently processing chunk number 120\n",
      "Currently processing chunk number 130\n",
      "Currently processing chunk number 140\n",
      "Currently processing chunk number 150\n",
      "Currently processing chunk number 160\n",
      "Currently processing chunk number 170\n",
      "Currently processing chunk number 180\n",
      "Currently processing chunk number 190\n",
      "Currently processing chunk number 200\n",
      "Currently processing chunk number 210\n",
      "Currently processing chunk number 220\n",
      "Currently processing chunk number 230\n",
      "Currently processing chunk number 240\n",
      "Currently processing chunk number 250\n",
      "Currently processing chunk number 260\n",
      "Currently processing chunk number 270\n",
      "Currently processing chunk number 280\n",
      "Currently processing chunk number 290\n",
      "Currently processing chunk number 300\n",
      "Currently processing chunk number 310\n",
      "Currently processing chunk number 320\n",
      "Currently processing chunk number 330\n",
      "Currently processing chunk number 340\n",
      "Currently processing chunk number 350\n",
      "Currently processing chunk number 360\n",
      "Currently processing chunk number 370\n",
      "Currently processing chunk number 380\n",
      "Currently processing chunk number 390\n",
      "Currently processing chunk number 400\n",
      "Currently processing chunk number 410\n",
      "Currently processing chunk number 420\n",
      "Currently processing chunk number 430\n",
      "Currently processing chunk number 440\n",
      "Currently processing chunk number 450\n",
      "Currently processing chunk number 460\n",
      "Currently processing chunk number 470\n",
      "Currently processing chunk number 480\n",
      "Currently processing chunk number 490\n",
      "Currently processing chunk number 500\n",
      "Currently processing chunk number 510\n",
      "Currently processing chunk number 520\n",
      "Currently processing chunk number 530\n",
      "Currently processing chunk number 540\n",
      "Currently processing chunk number 550\n",
      "Currently processing chunk number 560\n",
      "Currently processing chunk number 570\n",
      "Currently processing chunk number 580\n",
      "Currently processing chunk number 590\n",
      "Currently processing chunk number 600\n",
      "Currently processing chunk number 610\n",
      "Currently processing chunk number 620\n",
      "Currently processing chunk number 630\n",
      "Currently processing chunk number 640\n",
      "Currently processing chunk number 650\n",
      "Currently processing chunk number 660\n",
      "Currently processing chunk number 670\n",
      "Currently processing chunk number 680\n",
      "Currently processing chunk number 690\n",
      "Currently processing chunk number 700\n",
      "Currently processing chunk number 710\n",
      "Currently processing chunk number 720\n",
      "Currently processing chunk number 730\n",
      "Currently processing chunk number 740\n",
      "Currently processing chunk number 750\n",
      "Currently processing chunk number 760\n",
      "Currently processing chunk number 770\n",
      "Currently processing chunk number 780\n",
      "Currently processing chunk number 790\n",
      "Currently processing chunk number 800\n",
      "Currently processing chunk number 810\n",
      "Currently processing chunk number 820\n",
      "Currently processing chunk number 830\n",
      "Currently processing chunk number 840\n",
      "Currently processing chunk number 850\n",
      "Currently processing chunk number 860\n",
      "Currently processing chunk number 870\n",
      "Currently processing chunk number 880\n",
      "Currently processing chunk number 890\n",
      "Currently processing chunk number 900\n",
      "Currently processing chunk number 910\n",
      "Currently processing chunk number 920\n",
      "Currently processing chunk number 930\n",
      "Currently processing chunk number 940\n",
      "Currently processing chunk number 950\n",
      "Currently processing chunk number 960\n",
      "Currently processing chunk number 970\n",
      "Currently processing chunk number 980\n",
      "Currently processing chunk number 990\n",
      "Currently processing chunk number 1000\n",
      "Currently processing chunk number 1010\n",
      "Currently processing chunk number 1020\n",
      "Currently processing chunk number 1030\n",
      "Currently processing chunk number 1040\n",
      "Currently processing chunk number 1050\n",
      "Currently processing chunk number 1060\n",
      "Currently processing chunk number 1070\n",
      "Currently processing chunk number 1080\n",
      "Currently processing chunk number 1090\n",
      "Currently processing chunk number 1100\n",
      "Currently processing chunk number 1110\n",
      "Currently processing chunk number 1120\n",
      "Currently processing chunk number 1130\n",
      "Currently processing chunk number 1140\n",
      "Currently processing chunk number 1150\n",
      "Currently processing chunk number 1160\n",
      "Currently processing chunk number 1170\n",
      "Currently processing chunk number 1180\n",
      "Currently processing chunk number 1190\n",
      "Currently processing chunk number 1200\n",
      "Currently processing chunk number 1210\n",
      "Currently processing chunk number 1220\n",
      "Currently processing chunk number 1230\n",
      "Currently processing chunk number 1240\n",
      "Currently processing chunk number 1250\n",
      "Currently processing chunk number 1260\n",
      "Currently processing chunk number 1270\n",
      "Currently processing chunk number 1280\n",
      "Currently processing chunk number 1290\n",
      "Currently processing chunk number 1300\n",
      "Currently processing chunk number 1310\n",
      "Currently processing chunk number 1320\n",
      "Currently processing chunk number 1330\n",
      "Currently processing chunk number 1340\n",
      "Currently processing chunk number 1350\n",
      "Currently processing chunk number 1360\n",
      "Currently processing chunk number 1370\n",
      "Currently processing chunk number 1380\n",
      "Currently processing chunk number 1390\n",
      "Currently processing chunk number 1400\n",
      "Currently processing chunk number 1410\n",
      "Currently processing chunk number 1420\n",
      "Currently processing chunk number 1430\n",
      "Currently processing chunk number 1440\n",
      "Currently processing chunk number 1450\n",
      "Currently processing chunk number 1460\n",
      "Currently processing chunk number 1470\n",
      "Currently processing chunk number 1480\n",
      "Currently processing chunk number 1490\n",
      "Currently processing chunk number 1500\n",
      "Currently processing chunk number 1510\n",
      "Currently processing chunk number 1520\n",
      "Currently processing chunk number 1530\n",
      "Currently processing chunk number 1540\n",
      "Currently processing chunk number 1550\n",
      "Currently processing chunk number 1560\n",
      "Currently processing chunk number 1570\n",
      "Currently processing chunk number 1580\n",
      "Currently processing chunk number 1590\n",
      "Currently processing chunk number 1600\n",
      "Currently processing chunk number 1610\n",
      "Currently processing chunk number 1620\n",
      "Currently processing chunk number 1630\n",
      "Currently processing chunk number 1640\n",
      "Currently processing chunk number 1650\n",
      "Currently processing chunk number 1660\n",
      "Currently processing chunk number 1670\n",
      "Currently processing chunk number 1680\n",
      "Currently processing chunk number 1690\n",
      "Currently processing chunk number 1700\n",
      "Currently processing chunk number 1710\n",
      "Currently processing chunk number 1720\n",
      "Currently processing chunk number 1730\n",
      "Currently processing chunk number 1740\n",
      "Currently processing chunk number 1750\n",
      "Currently processing chunk number 1760\n",
      "Currently processing chunk number 1770\n",
      "Currently processing chunk number 1780\n",
      "Currently processing chunk number 1790\n",
      "Currently processing chunk number 1800\n",
      "Currently processing chunk number 1810\n",
      "Currently processing chunk number 1820\n",
      "Currently processing chunk number 1830\n",
      "Currently processing chunk number 1840\n",
      "Currently processing chunk number 1850\n",
      "Currently processing chunk number 1860\n",
      "Currently processing chunk number 1870\n",
      "Currently processing chunk number 1880\n",
      "Currently processing chunk number 1890\n",
      "Currently processing chunk number 1900\n",
      "Currently processing chunk number 1910\n",
      "Currently processing chunk number 1920\n",
      "Currently processing chunk number 1930\n",
      "Currently processing chunk number 1940\n",
      "Currently processing chunk number 1950\n",
      "Currently processing chunk number 1960\n",
      "Currently processing chunk number 1970\n",
      "Currently processing chunk number 1980\n",
      "Currently processing chunk number 1990\n",
      "Currently processing chunk number 2000\n",
      "Currently processing chunk number 2010\n",
      "Currently processing chunk number 2020\n",
      "Currently processing chunk number 2030\n",
      "Currently processing chunk number 2040\n",
      "Currently processing chunk number 2050\n",
      "Currently processing chunk number 2060\n",
      "Currently processing chunk number 2070\n",
      "Currently processing chunk number 2080\n",
      "Currently processing chunk number 2090\n",
      "Currently processing chunk number 2100\n",
      "Currently processing chunk number 2110\n",
      "Currently processing chunk number 2120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 2130\n",
      "Currently processing chunk number 2140\n",
      "Currently processing chunk number 2150\n",
      "Currently processing chunk number 2160\n",
      "Currently processing chunk number 2170\n",
      "Currently processing chunk number 2180\n",
      "Currently processing chunk number 2190\n",
      "Currently processing chunk number 2200\n",
      "Currently processing chunk number 2210\n",
      "Currently processing chunk number 2220\n",
      "Currently processing chunk number 2230\n",
      "Currently processing chunk number 2240\n",
      "Currently processing chunk number 2250\n",
      "Currently processing chunk number 2260\n",
      "Currently processing chunk number 2270\n",
      "Currently processing chunk number 2280\n",
      "Currently processing chunk number 2290\n",
      "Currently processing chunk number 2300\n",
      "Currently processing chunk number 2310\n",
      "Currently processing chunk number 2320\n",
      "Currently processing chunk number 2330\n",
      "Currently processing chunk number 2340\n",
      "Currently processing chunk number 2350\n",
      "Currently processing chunk number 2360\n",
      "Currently processing chunk number 2370\n",
      "Currently processing chunk number 2380\n",
      "Currently processing chunk number 2390\n",
      "Currently processing chunk number 2400\n",
      "Currently processing chunk number 2410\n",
      "Currently processing chunk number 2420\n",
      "Currently processing chunk number 2430\n",
      "Currently processing chunk number 2440\n",
      "Currently processing chunk number 2450\n",
      "Currently processing chunk number 2460\n",
      "Currently processing chunk number 2470\n",
      "Currently processing chunk number 2480\n",
      "Currently processing chunk number 2490\n",
      "Currently processing chunk number 2500\n",
      "Currently processing chunk number 2510\n",
      "Currently processing chunk number 2520\n",
      "Currently processing chunk number 2530\n",
      "Currently processing chunk number 2540\n",
      "Currently processing chunk number 2550\n",
      "Currently processing chunk number 2560\n",
      "Currently processing chunk number 2570\n",
      "Currently processing chunk number 2580\n",
      "Currently processing chunk number 2590\n",
      "Currently processing chunk number 2600\n",
      "Currently processing chunk number 2610\n",
      "Currently processing chunk number 2620\n",
      "Currently processing chunk number 2630\n",
      "Currently processing chunk number 2640\n",
      "Currently processing chunk number 2650\n",
      "Currently processing chunk number 2660\n",
      "Currently processing chunk number 2670\n",
      "Currently processing chunk number 2680\n",
      "Currently processing chunk number 2690\n",
      "Currently processing chunk number 2700\n",
      "Currently processing chunk number 2710\n",
      "Currently processing chunk number 2720\n",
      "Currently processing chunk number 2730\n",
      "Currently processing chunk number 2740\n",
      "Currently processing chunk number 2750\n",
      "Currently processing chunk number 2760\n",
      "Currently processing chunk number 2770\n",
      "Currently processing chunk number 2780\n",
      "Currently processing chunk number 2790\n",
      "Currently processing chunk number 2800\n",
      "Currently processing chunk number 2810\n",
      "Currently processing chunk number 2820\n",
      "Currently processing chunk number 2830\n",
      "Currently processing chunk number 2840\n",
      "Currently processing chunk number 2850\n",
      "Currently processing chunk number 2860\n",
      "Currently processing chunk number 2870\n",
      "Currently processing chunk number 2880\n",
      "Currently processing chunk number 2890\n",
      "Currently processing chunk number 2900\n",
      "Currently processing chunk number 2910\n",
      "Currently processing chunk number 2920\n",
      "Currently processing chunk number 2930\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 2934, this means the last chunk to be run was chunk 2933\n",
      "Can only use .str accessor with string values!\n",
      "Currently processing chunk number 2940\n",
      "Currently processing chunk number 2950\n",
      "Currently processing chunk number 2960\n",
      "Currently processing chunk number 2970\n",
      "Currently processing chunk number 2980\n",
      "Currently processing chunk number 2990\n",
      "Currently processing chunk number 3000\n",
      "Currently processing chunk number 3010\n",
      "Currently processing chunk number 3020\n",
      "Currently processing chunk number 3030\n",
      "Currently processing chunk number 3040\n",
      "Currently processing chunk number 3050\n",
      "Currently processing chunk number 3060\n",
      "Currently processing chunk number 3070\n",
      "Currently processing chunk number 3080\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Error encountered on chunk 3083, this means the last chunk to be run was chunk 3082\n",
      "Can only use .str accessor with string values!\n",
      "Currently processing chunk number 3090\n",
      "Currently processing chunk number 3100\n",
      "Currently processing chunk number 3110\n",
      "Currently processing chunk number 3120\n",
      "Currently processing chunk number 3130\n",
      "Currently processing chunk number 3140\n",
      "Currently processing chunk number 3150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 3160\n",
      "Currently processing chunk number 3170\n",
      "Currently processing chunk number 3180\n",
      "Currently processing chunk number 3190\n",
      "Currently processing chunk number 3200\n",
      "Currently processing chunk number 3210\n",
      "Currently processing chunk number 3220\n",
      "Currently processing chunk number 3230\n",
      "Currently processing chunk number 3240\n",
      "Currently processing chunk number 3250\n",
      "Currently processing chunk number 3260\n",
      "Currently processing chunk number 3270\n",
      "Currently processing chunk number 3280\n",
      "Currently processing chunk number 3290\n",
      "Currently processing chunk number 3300\n",
      "Currently processing chunk number 3310\n",
      "Currently processing chunk number 3320\n",
      "Currently processing chunk number 3330\n",
      "Currently processing chunk number 3340\n",
      "Currently processing chunk number 3350\n",
      "Currently processing chunk number 3360\n",
      "Currently processing chunk number 3370\n",
      "Currently processing chunk number 3380\n",
      "Currently processing chunk number 3390\n",
      "Currently processing chunk number 3400\n",
      "Currently processing chunk number 3410\n",
      "Currently processing chunk number 3420\n",
      "Currently processing chunk number 3430\n",
      "Currently processing chunk number 3440\n",
      "Currently processing chunk number 3450\n",
      "Currently processing chunk number 3460\n",
      "Currently processing chunk number 3470\n",
      "Currently processing chunk number 3480\n",
      "Currently processing chunk number 3490\n",
      "Currently processing chunk number 3500\n",
      "Currently processing chunk number 3510\n",
      "Currently processing chunk number 3520\n",
      "Currently processing chunk number 3530\n",
      "Currently processing chunk number 3540\n",
      "Currently processing chunk number 3550\n",
      "Currently processing chunk number 3560\n",
      "Currently processing chunk number 3570\n",
      "Currently processing chunk number 3580\n",
      "Currently processing chunk number 3590\n",
      "Currently processing chunk number 3600\n",
      "Currently processing chunk number 3610\n",
      "Currently processing chunk number 3620\n",
      "Currently processing chunk number 3630\n",
      "Currently processing chunk number 3640\n",
      "Currently processing chunk number 3650\n",
      "Currently processing chunk number 3660\n",
      "Currently processing chunk number 3670\n",
      "Currently processing chunk number 3680\n",
      "Currently processing chunk number 3690\n",
      "Currently processing chunk number 3700\n",
      "Currently processing chunk number 3710\n",
      "Currently processing chunk number 3720\n",
      "Currently processing chunk number 3730\n",
      "Currently processing chunk number 3740\n",
      "Currently processing chunk number 3750\n",
      "Currently processing chunk number 3760\n",
      "Currently processing chunk number 3770\n",
      "Currently processing chunk number 3780\n",
      "Currently processing chunk number 3790\n",
      "Currently processing chunk number 3800\n",
      "Currently processing chunk number 3810\n",
      "Currently processing chunk number 3820\n",
      "Currently processing chunk number 3830\n",
      "Currently processing chunk number 3840\n",
      "Currently processing chunk number 3850\n",
      "Currently processing chunk number 3860\n",
      "Currently processing chunk number 3870\n",
      "Currently processing chunk number 3880\n",
      "Currently processing chunk number 3890\n",
      "Currently processing chunk number 3900\n",
      "Currently processing chunk number 3910\n",
      "Currently processing chunk number 3920\n",
      "Currently processing chunk number 3930\n",
      "Currently processing chunk number 3940\n",
      "Currently processing chunk number 3950\n",
      "Currently processing chunk number 3960\n",
      "Currently processing chunk number 3970\n",
      "Currently processing chunk number 3980\n",
      "Currently processing chunk number 3990\n",
      "Currently processing chunk number 4000\n",
      "Currently processing chunk number 4010\n",
      "Currently processing chunk number 4020\n",
      "Currently processing chunk number 4030\n",
      "Currently processing chunk number 4040\n",
      "Currently processing chunk number 4050\n",
      "Currently processing chunk number 4060\n",
      "Currently processing chunk number 4070\n",
      "Currently processing chunk number 4080\n",
      "Currently processing chunk number 4090\n",
      "Currently processing chunk number 4100\n",
      "Currently processing chunk number 4110\n",
      "Currently processing chunk number 4120\n",
      "Currently processing chunk number 4130\n",
      "Currently processing chunk number 4140\n",
      "Currently processing chunk number 4150\n",
      "Currently processing chunk number 4160\n",
      "Currently processing chunk number 4170\n",
      "Currently processing chunk number 4180\n",
      "Currently processing chunk number 4190\n",
      "Currently processing chunk number 4200\n",
      "Currently processing chunk number 4210\n",
      "Currently processing chunk number 4220\n",
      "Currently processing chunk number 4230\n",
      "Currently processing chunk number 4240\n",
      "Currently processing chunk number 4250\n",
      "Currently processing chunk number 4260\n",
      "Currently processing chunk number 4270\n",
      "Currently processing chunk number 4280\n",
      "Currently processing chunk number 4290\n",
      "Currently processing chunk number 4300\n",
      "Currently processing chunk number 4310\n",
      "Currently processing chunk number 4320\n",
      "Currently processing chunk number 4330\n",
      "Currently processing chunk number 4340\n",
      "Currently processing chunk number 4350\n",
      "Currently processing chunk number 4360\n",
      "Currently processing chunk number 4370\n",
      "Currently processing chunk number 4380\n",
      "Currently processing chunk number 4390\n",
      "Currently processing chunk number 4400\n",
      "Currently processing chunk number 4410\n",
      "Currently processing chunk number 4420\n",
      "Currently processing chunk number 4430\n",
      "Currently processing chunk number 4440\n",
      "Currently processing chunk number 4450\n",
      "Currently processing chunk number 4460\n",
      "Currently processing chunk number 4470\n",
      "Currently processing chunk number 4480\n",
      "Currently processing chunk number 4490\n",
      "Currently processing chunk number 4500\n",
      "Currently processing chunk number 4510\n",
      "Currently processing chunk number 4520\n",
      "Currently processing chunk number 4530\n",
      "Currently processing chunk number 4540\n",
      "Currently processing chunk number 4550\n",
      "Currently processing chunk number 4560\n",
      "Currently processing chunk number 4570\n",
      "Currently processing chunk number 4580\n",
      "Currently processing chunk number 4590\n",
      "Currently processing chunk number 4600\n",
      "Currently processing chunk number 4610\n",
      "Currently processing chunk number 4620\n",
      "Currently processing chunk number 4630\n",
      "Currently processing chunk number 4640\n",
      "Currently processing chunk number 4650\n",
      "Currently processing chunk number 4660\n",
      "Currently processing chunk number 4670\n",
      "Currently processing chunk number 4680\n",
      "Currently processing chunk number 4690\n",
      "Currently processing chunk number 4700\n",
      "Currently processing chunk number 4710\n",
      "Currently processing chunk number 4720\n",
      "Currently processing chunk number 4730\n",
      "Currently processing chunk number 4740\n",
      "Currently processing chunk number 4750\n",
      "Currently processing chunk number 4760\n",
      "Currently processing chunk number 4770\n",
      "Currently processing chunk number 4780\n",
      "Currently processing chunk number 4790\n",
      "Currently processing chunk number 4800\n",
      "Currently processing chunk number 4810\n",
      "Currently processing chunk number 4820\n",
      "Currently processing chunk number 4830\n",
      "Currently processing chunk number 4840\n",
      "Currently processing chunk number 4850\n",
      "Currently processing chunk number 4860\n",
      "Currently processing chunk number 4870\n",
      "Currently processing chunk number 4880\n",
      "Currently processing chunk number 4890\n",
      "Currently processing chunk number 4900\n",
      "Currently processing chunk number 4910\n",
      "Currently processing chunk number 4920\n",
      "Currently processing chunk number 4930\n",
      "Currently processing chunk number 4940\n",
      "Currently processing chunk number 4950\n",
      "Currently processing chunk number 4960\n",
      "Currently processing chunk number 4970\n",
      "Currently processing chunk number 4980\n",
      "Currently processing chunk number 4990\n",
      "Currently processing chunk number 5000\n",
      "Currently processing chunk number 5010\n",
      "Currently processing chunk number 5020\n",
      "Currently processing chunk number 5030\n",
      "Currently processing chunk number 5040\n",
      "Currently processing chunk number 5050\n",
      "Currently processing chunk number 5060\n",
      "Currently processing chunk number 5070\n",
      "Currently processing chunk number 5080\n",
      "Currently processing chunk number 5090\n",
      "Currently processing chunk number 5100\n",
      "Currently processing chunk number 5110\n",
      "Currently processing chunk number 5120\n",
      "Currently processing chunk number 5130\n",
      "Currently processing chunk number 5140\n",
      "Currently processing chunk number 5150\n",
      "Currently processing chunk number 5160\n",
      "Currently processing chunk number 5170\n",
      "Currently processing chunk number 5180\n",
      "Currently processing chunk number 5190\n",
      "Currently processing chunk number 5200\n",
      "Currently processing chunk number 5210\n",
      "Currently processing chunk number 5220\n",
      "Currently processing chunk number 5230\n",
      "Currently processing chunk number 5240\n",
      "Currently processing chunk number 5250\n",
      "Currently processing chunk number 5260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 5270\n",
      "Currently processing chunk number 5280\n",
      "Currently processing chunk number 5290\n",
      "Currently processing chunk number 5300\n",
      "Currently processing chunk number 5310\n",
      "Currently processing chunk number 5320\n",
      "Currently processing chunk number 5330\n",
      "Currently processing chunk number 5340\n",
      "Currently processing chunk number 5350\n",
      "Currently processing chunk number 5360\n",
      "Currently processing chunk number 5370\n",
      "Currently processing chunk number 5380\n",
      "Currently processing chunk number 5390\n",
      "Currently processing chunk number 5400\n",
      "Currently processing chunk number 5410\n",
      "Currently processing chunk number 5420\n",
      "Currently processing chunk number 5430\n",
      "Currently processing chunk number 5440\n",
      "Currently processing chunk number 5450\n",
      "Currently processing chunk number 5460\n",
      "Currently processing chunk number 5470\n",
      "Currently processing chunk number 5480\n",
      "Currently processing chunk number 5490\n",
      "Currently processing chunk number 5500\n",
      "Currently processing chunk number 5510\n",
      "Currently processing chunk number 5520\n",
      "Currently processing chunk number 5530\n",
      "Currently processing chunk number 5540\n",
      "Currently processing chunk number 5550\n",
      "Currently processing chunk number 5560\n",
      "Currently processing chunk number 5570\n",
      "Currently processing chunk number 5580\n",
      "Currently processing chunk number 5590\n",
      "Currently processing chunk number 5600\n",
      "Currently processing chunk number 5610\n",
      "Currently processing chunk number 5620\n",
      "Currently processing chunk number 5630\n",
      "Currently processing chunk number 5640\n",
      "Currently processing chunk number 5650\n",
      "Currently processing chunk number 5660\n",
      "Currently processing chunk number 5670\n",
      "Currently processing chunk number 5680\n",
      "Currently processing chunk number 5690\n",
      "Currently processing chunk number 5700\n",
      "Currently processing chunk number 5710\n",
      "Currently processing chunk number 5720\n",
      "Currently processing chunk number 5730\n",
      "Currently processing chunk number 5740\n",
      "Currently processing chunk number 5750\n"
     ]
    }
   ],
   "source": [
    "cp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927136be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
