{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3778eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "import usaddress\n",
    "\n",
    "from difflib import get_close_matches as clmatch\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely import Point\n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "\n",
    "path = pathlib.Path().resolve()\n",
    "\n",
    "data_path = path.parent / \"Dropbox\" / \"2019 MV Data by Town\" / \"Vehicles_2022\" / \"Compiled\"\n",
    "\n",
    "raw_data = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", chunksize = 1000)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f6ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = 'municipal_data_prep.log', level = logging.INFO,  format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb256647",
   "metadata": {},
   "source": [
    "# Full processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0381bf6c",
   "metadata": {},
   "source": [
    "## Prepare data and import matching sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80bd6764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajan\\AppData\\Local\\Temp\\ipykernel_5784\\143037737.py:2: DtypeWarning: Columns (7,9,10,13,20,25,30,34,42,44,56,58,68,78,80,114,115,138,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nhtsa_cleaned = pd.read_csv(path / \"ignored-data\" / \"NHTSA_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load NHTSA data\n",
    "nhtsa_cleaned = pd.read_csv(path / \"ignored-data\" / \"NHTSA_cleaned.csv\")\n",
    "\n",
    "# Simplify the cleaned file\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned[[\"VIN\", \"Manufacturer\", \"Model\", \"ModelYear\", \"FuelTypePrimary\", \"ElectrificationLevel\"]]\n",
    "nhtsa_cleaned_simple = nhtsa_cleaned_simple.rename(columns = {\"VIN\":\"vin_corrected\",\n",
    "                                                              \"Manufacturer\" : \"Manufacturer Name\",\n",
    "                                                              \"ModelYear\" : \"Model Year\",\n",
    "                                                              \"FuelTypePrimary\" : \"Fuel Type - Primary\",\n",
    "                                                              \"ElectrificationLevel\" : \"Electrification Level\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c37a08",
   "metadata": {},
   "source": [
    "## Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkProcessor():\n",
    "    def __init__(self, chunks, processed_chunk_path, number_to_run, zip_processor, vin_matcher, zip_column_name, vin_column_name):\n",
    "        self.chunks = chunks\n",
    "        self.processed_chunks = pd.DataFrame([])\n",
    "        self.processed_chunks_path = processed_chunk_path\n",
    "        self.number_to_run = number_to_run\n",
    "        self.chunk_number = 0\n",
    "        self.zip_processor = zip_processor\n",
    "        self.vin_matcher = vin_matcher\n",
    "        self.zip_column_name = zip_column_name\n",
    "        self.vin_column_name = vin_column_name\n",
    "        \n",
    "    def run(self):\n",
    "        for chunk in self.chunks:\n",
    "            if self.chunk_number < self.number_to_run:\n",
    "                chunk_processed = self.process_chunk(chunk)\n",
    "            else:\n",
    "                logging.info(f\"Concluding on chunk number {self.chunk_number}\")\n",
    "    \n",
    "    def process_chunk(self, chunk):\n",
    "        try:\n",
    "            # Give progress\n",
    "            if self.chunk_number % 1 == 0:\n",
    "                print(f\"Currently processing chunk number {self.chunk_number}\")\n",
    "            \n",
    "            # Log progress\n",
    "            logging.info(f\"Chunk Number {self.chunk_number}: commencing processing\")\n",
    "            logging.info(f\"Chunk Number {self.chunk_number}: chunk length is {len(chunk)}\")\n",
    "        \n",
    "            # Reduce reduce the number of columns\n",
    "            chunk_simplified = chunk[['record_from', 'name', 'street', 'city', \n",
    "                                  'state', 'zip', 'vehicle_year', 'vehicle_make', 'vehicle_model',\n",
    "                                  'vehicle_class', 'vehicle_id']].reset_index(drop = True)\n",
    "        \n",
    "            # Correct the zip codes\n",
    "            corrected_zip_codes = self.zip_processor.get_valid_zips(chunk_simplified, self.zip_column_name).reset_index(drop = True)\n",
    "\n",
    "            # Get VIN codes\n",
    "            matched_vins = self.vin_matcher.match_vins(chunk, self.vin_column_name).reset_index(drop=True)\n",
    "\n",
    "            # Merge\n",
    "            chunk_processed = chunk_simplified.join(corrected_zip_codes).reset_index(drop=True)\n",
    "            chunk_processed = chunk_processed.join(matched_vins)\n",
    "            \n",
    "            # Aggregate and save\n",
    "            self.aggregate_save_chunk(chunk_processed)\n",
    "\n",
    "            # Update chunk number\n",
    "            self.chunk_number +=1\n",
    "            \n",
    "            return chunk_processed\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error encountered on chunk {i}, this means the last chunk to be run was chunk {i-1}\")\n",
    "            logging.error(e)\n",
    "            print(f\"Error encountered on chunk {i}, this means the last chunk to be run was chunk {i-1}\")\n",
    "            print(e)\n",
    "        \n",
    "    def aggregate_save_chunk(self, processed_chunk):\n",
    "        # Add it to the master DF\n",
    "        self.processed_chunks = pd.concat([self.processed_chunks, processed_chunk])\n",
    "        \n",
    "        if self.chunk_number % 10 == 0:\n",
    "            logging.info(f\"Saving aggregated chunks numbers {self.chunk_number-100} - {self.chunk_number}\")\n",
    "            self.processed_chunks.to_csv(self.processed_chunks_path / f\"chunk_number_{self.chunk_number}.csv\")\n",
    "            \n",
    "            # Once saved, reset the DF\n",
    "            self.processed_chunks = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26286cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZIPProcessor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def check_valid_zip(self, zip_code):\n",
    "        zip_str = str(zip_code)\n",
    "        split_zip = re.split(\"-\", zip_str)\n",
    "        if len(split_zip) == 2:\n",
    "            if self.check_valid_zip(split_zip[0]) & self.check_valid_zip(split_zip[1]):\n",
    "                return 2\n",
    "            elif self.check_valid_zip(split_zip[0]):\n",
    "                return 3\n",
    "            else:\n",
    "                return 0\n",
    "        elif len(split_zip) == 1:\n",
    "            # MUST ADDRESS STARTING \"Os\"\n",
    "            # starting_o = re.match('^O', split_zip[0])\n",
    "            matched = re.match(\"^\\s*[0-9]*[0-9]{4}\\.?0?\\s*$\", split_zip[0])\n",
    "            if matched:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_valid_zips(self, zip_df, zip_column_name):\n",
    "        \"\"\"\n",
    "        Inputs: A DataFrame with a column called \"zip\"\n",
    "        Returns: A DataFrame of the same length as the input, with three columns: zip, zip_valid_code, and zip_corrected\n",
    "        \"\"\"\n",
    "        # Prepare the list to be used\n",
    "        zip_list = zip_df[[zip_column_name]].rename(columns = {zip_column_name : \"zip\"}).reset_index(drop = True)\n",
    "\n",
    "        # Get validity code\n",
    "        zip_list.loc[:, \"zip_valid_code\"] = zip_list.loc[:, \"zip\"].apply(lambda x: self.check_valid_zip(x))\n",
    "\n",
    "        # Get indices\n",
    "        correct_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==1].index\n",
    "        invalid_zips_indices = zip_list[zip_list[\"zip_valid_code\"]==0].index\n",
    "        two_part_zips_indices = zip_list[zip_list[\"zip_valid_code\"]>1].index\n",
    "\n",
    "        zip_list.loc[correct_zips_indices, \"zip_corrected\"] = zip_list.loc[correct_zips_indices, \"zip\"]\n",
    "        zip_list.loc[invalid_zips_indices, \"zip_corrected\"] = np.NaN\n",
    "        zip_list.loc[two_part_zips_indices, \"zip_corrected\"] = zip_list.loc[two_part_zips_indices, \"zip\"].astype(str).str[0:5]\n",
    "\n",
    "        return zip_list[[\"zip_corrected\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05385113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VINMatcher():\n",
    "    def __init__(self, initial_matching_list):\n",
    "        self.matching_list = initial_matching_list\n",
    "        self.queries = 0\n",
    "        \n",
    "    def valid_vins(self, vins_to_correct_df, vin_column):\n",
    "        \"\"\"\n",
    "        Input: A DataFrame containing a \"vehicle_id\" column of VINs to be corrected\n",
    "        Output: A DataFrame with two columns: \"vehicle_id\" and \"vin_corrected\"\n",
    "        \"\"\"\n",
    "        # Set up\n",
    "        vin_list = vins_to_correct_df\n",
    "        \n",
    "        # Check all alphanumerics\n",
    "        vin_list.loc[:, \"vin_alnum_check\"] = vin_list[vin_column].str.strip().str.isalnum()\n",
    "        vin_list.loc[:, \"vin_len_check\"] = vin_list[vin_column].str.len() >= 11\n",
    "        vin_list.loc[:, \"vin_check\"] = vin_list[\"vin_alnum_check\"] & vin_list[\"vin_len_check\"]\n",
    "\n",
    "        # Strip the vins\n",
    "        vin_list[\"vin_stripped\"] = vin_list[vin_column].str.strip()\n",
    "\n",
    "        # Create 11-long vins\n",
    "        vin_list.loc[:, \"prepared_vins\"] = vin_list[vin_column].str[0:8]+\"*\"+vin_list[vin_column].str[9:11]\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==False].index, \"vin_corrected\"] = np.NaN\n",
    "        vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"vin_corrected\"] = vin_list.loc[vin_list[vin_list[\"vin_check\"]==True].index, \"prepared_vins\"]\n",
    "\n",
    "        # Clean up\n",
    "        vin_list = vin_list.drop(\"prepared_vins\", axis = 1)\n",
    "        vin_list = vin_list[[\"vin_corrected\"]]\n",
    "\n",
    "        # Return\n",
    "        return vin_list\n",
    "        \n",
    "    def match_vins(self, df, vin_column):\n",
    "        \"\"\"\n",
    "        Input: A df containing vin columns, that are then corrected, and matched\n",
    "        Returns: matched vins, updated matching list    \n",
    "        \"\"\"\n",
    "        # Get a list of valid VINs\n",
    "        valid_vin_list = self.valid_vins(df[[vin_column]], vin_column)\n",
    "        \n",
    "        # Attempt a match\n",
    "        match = valid_vin_list.merge(self.matching_list,\n",
    "                                     left_on = \"vin_corrected\",\n",
    "                                     right_on = \"vin_corrected\",\n",
    "                                     how = 'left')\n",
    "        # Get unique unmatched vins\n",
    "        unmatched_vins = list(match[match[\"Manufacturer Name\"].isna()][\"vin_corrected\"].unique())\n",
    "\n",
    "        # Print how many\n",
    "        logging.info(f\"VIN matching: a total of {len(unmatched_vins)} VINs were not matched\")\n",
    "\n",
    "        # Variables to download\n",
    "        variables = [\"Manufacturer Name\", \"Model\", \"Model Year\", \"Fuel Type - Primary\", \"Electrification Level\"]\n",
    "        \n",
    "        # Go get them\n",
    "        for vin in tqdm(unmatched_vins):\n",
    "            try:\n",
    "                # Try to fetch the unmatched vin\n",
    "                resp_df = self.fetch_unmatched_vin(vin).reset_index(drop=True)\n",
    "                for variable in variables:\n",
    "                    match.loc[match[match[\"vin_corrected\"]==vin].index, match.columns.isin([variable])] = resp_df[variable][0]\n",
    "                \n",
    "            except BaseException as e:\n",
    "                # print(e)\n",
    "                logging.info(e)\n",
    "                pass\n",
    "\n",
    "        remaining_unmatched = list(match[match[\"Manufacturer Name\"].isna()][\"vin_corrected\"].unique())\n",
    "        \n",
    "        logging.info(f\"VIN Matching: this number of unmatched VINs was reduced by {len(unmatched_vins) - len(remaining_unmatched)}\")\n",
    "        logging.info(f\"VIN Matching: remaining unmatched VINs is {len(remaining_unmatched)}\")\n",
    "\n",
    "        return match\n",
    "    \n",
    "    def fetch_unmatched_vin(self, unmatched_vin):\n",
    "        \"\"\"\n",
    "        Input: An unmatched, but corrected VIN\n",
    "        Output: A matched VIN or NA\n",
    "        \n",
    "        \"\"\"\n",
    "        # Increment the number of times queried\n",
    "        self.queries +=1\n",
    "        \n",
    "        variables = [\"Manufacturer Name\", \"Model\", \"Model Year\", \"Fuel Type - Primary\", \"Electrification Level\"]\n",
    "        \n",
    "        url = (f\"https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVin/{unmatched_vin.strip()}?format=csv\")\n",
    "\n",
    "        # Download response\n",
    "        resp_df = pd.read_csv(url)\n",
    "\n",
    "        # Extract needed\n",
    "        resp_df = resp_df.loc[resp_df[\"variable\"].isin(variables), [\"variable\", \"value\"]].T\n",
    "        resp_df.columns = resp_df.iloc[0]\n",
    "        resp_df = resp_df.drop(\"variable\", axis = 0)\n",
    "        resp_df[\"vin_corrected\"] = unmatched_vin\n",
    "        valid_response = not(resp_df[\"Fuel Type - Primary\"].isna()[0])\n",
    "        \n",
    "        # Update the matching list\n",
    "        self.matching_list = pd.concat([self.matching_list, resp_df]).reset_index(drop = True)\n",
    "        \n",
    "        # If the number of queries is an increment of 100, save the matching list        \n",
    "        if self.queries % 10 == 0:\n",
    "            logging.info(f\"Saving matching list after {self.queries} queries\")\n",
    "            self.matching_list.to_csv(path / \"matching_list.csv\")\n",
    "        \n",
    "        return resp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284582b",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks_path = path/\"processed_chunks\"\n",
    "number_to_run = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffd3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(data_path / \"2019-21_data_compiled_RN_100323.csv\", chunksize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a08e9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "zp = ZIPProcessor()\n",
    "vm = VINMatcher(nhtsa_cleaned_simple)\n",
    "cp = ChunkProcessor(chunks, processed_chunks_path, number_to_run, zp, vm, \"zip\", \"vehicle_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1fd24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:03<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing chunk number 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mChunkProcessor.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_number \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_to_run:\n\u001b[0;32m     16\u001b[0m             chunk_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_chunk(chunk)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1187\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1284\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:817\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1038\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1429\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1425\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1426\u001b[0m     )\n\u001b[1;32m-> 1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
